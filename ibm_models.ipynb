{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def tokenize(lines):\n",
    "    lines = [line.rstrip() for line in lines[:nr_used_sentences]] #remove \\n\n",
    "    words = [line.split() for line in lines] #split by words\n",
    "    unpunctuated = str.maketrans('', '', string.punctuation) #remove punctuation & make Lowercase\n",
    "    lower = [[word.translate(unpunctuated).lower() for word in sentence] for sentence in words]\n",
    "    return lower\n",
    "\n",
    "def read_in_corpus(nr_used_sentences, corpus_path):\n",
    "    filename_e = \"BU_en.txt\"\n",
    "    filename_f = \"BU_tr.txt\"\n",
    "\n",
    "    e_file = open(corpus_path + filename_e, \"r\", encoding='utf-8-sig')\n",
    "    e_lines = e_file.readlines()\n",
    "    f_file = open(corpus_path + filename_f, \"r\", encoding='utf-8-sig')\n",
    "    f_lines = f_file.readlines()\n",
    "\n",
    "    f = tokenize(f_lines)\n",
    "    e = tokenize(e_lines)\n",
    "    return e, f\n",
    "\n",
    "nr_used_sentences = 30 #of 688670 lines\n",
    "path = \"./Paralel Corpus/\"\n",
    "e, f = read_in_corpus(nr_used_sentences, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "max_steps = 50\n",
    "training_split = 0.9\n",
    "\n",
    "def split_dataset(dataset, training_split):\n",
    "    np.random.shuffle(dataset)\n",
    "    nr_train_samples = int(training_split*len(dataset))\n",
    "    return dataset[:nr_train_samples], dataset[nr_train_samples:]\n",
    "\n",
    "e_train, e_test = split_dataset(e, training_split)\n",
    "f_train, f_test = split_dataset(f, training_split)\n",
    "e_train = [['house','the','the'],['the','the','book'],['a','book']]\n",
    "f_train = [['das','Haus'],['das','Buch'],['ein','Buch']]\n",
    "\n",
    "e_words = set([item for sublist in e_train for item in sublist])\n",
    "f_words = set([item for sublist in f_train for item in sublist])\n",
    "    \n",
    "#test if the two sets are equivalent\n",
    "def is_converged(t, last_t):\n",
    "    return t == last_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['house', 'the', 'the'], ['the', 'the', 'book'], ['a', 'book']]\n",
      "0.333333333333345 0.011414945271378608\n",
      "0.6666666666666549 0.9999999999994895\n",
      "0.014902246322072639 1e-12\n",
      "0.9850977536779216 0.9961746545566007\n",
      "5.696228382142683e-15 1e-12\n",
      "1.0 0.9885850547281109\n",
      "1.0063233751843367e-25 1e-12\n",
      "2.9688175631860936e-18 1e-12\n",
      "0.017831796971375664 0.0038253454429096182\n",
      "0.9821682030286244 0.9999999999994867\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "#learning translation probabilitity distributions from sentence-aligned parallel text\n",
    "#expectation maximization algorithm\n",
    "#Input: set of sentence pairs (e,f) \n",
    "#Output: translation prob. t(e|f)\n",
    "#S.91 Figure 4.3\n",
    "def EM_IBM_Model_1 (e_set,f_set):\n",
    "    #sets of distinct words\n",
    "    e_words = set([item for sublist in e_set for item in sublist])\n",
    "    f_words = set([item for sublist in f_set for item in sublist])\n",
    "    #initialize t(e|f) with uniform distribution\n",
    "    t = {}#{key: {key2: 1/len(e_words) for key2 in f_words} for key in e_words} #make code faster\n",
    "    for k in range (0, len(e_set)):\n",
    "        e = e_set[k]\n",
    "        f = f_set[k]\n",
    "        for e_j in e:\n",
    "            for f_i in f:\n",
    "                t[(e_j,f_i)] = 1/len(e_words) #TODO is this right? divide by 4\n",
    "    last_t = {0:1} #to fail first comparison\n",
    "    step = 0\n",
    "    #iterate until convergence\n",
    "    while not(is_converged(t, last_t)) and step < max_steps:\n",
    "        #initialize\n",
    "        count = {(e,f): 0 for f in f_words for e in e_words}\n",
    "        total = {f: 0 for f in f_words}\n",
    "        for k in range (0, len(e_set)):#works cause number lines same\n",
    "            e = e_set[k]\n",
    "            f = f_set[k]\n",
    "            #compute normalization\n",
    "            #s_total = {e: 0 for e in e_words}\n",
    "            s_total = {}\n",
    "            for e_j in e:\n",
    "                s_total[e_j] = 0\n",
    "                for f_i in f:\n",
    "                    s_total[e_j] += t[(e_j,f_i)]\n",
    "            #collect counts\n",
    "            for f_i in f:\n",
    "                for e_j in e:\n",
    "                    count[(e_j,f_i)] += t[(e_j,f_i)] / s_total[e_j]\n",
    "                    total[f_i] += t[(e_j,f_i)] / s_total[e_j]\n",
    "        #estimate probabilities\n",
    "        last_t = copy.deepcopy(t)\n",
    "        for f_i in f_words:\n",
    "            for e_j in e_words:\n",
    "                if (e_j,f_i) in t:\n",
    "                    t[(e_j,f_i)] = count[(e_j,f_i)] / total[f_i] \n",
    "        step += 1\n",
    "        #print(last_t)\n",
    "        #print('---')\n",
    "    return t\n",
    "print(e_train)\n",
    "t = EM_IBM_Model_1(e_train,f_train)\n",
    "#print(t)\n",
    "\n",
    "#compare t table to IBM Model 1 implementation of nltk library\n",
    "import nltk\n",
    "from nltk.translate import AlignedSent, IBMModel1\n",
    "bitext = []\n",
    "bitext.append(AlignedSent(['das', 'Haus'], ['house','the','the']))\n",
    "bitext.append(AlignedSent(['das', 'Buch'], ['the', 'the','book']))\n",
    "bitext.append(AlignedSent(['ein', 'Buch'], ['a', 'book']))\n",
    "ibm1 = IBMModel1(bitext, 50)\n",
    "for f_i in f_words:\n",
    "    for e_j in e_words:\n",
    "        if (e_j,f_i) in t:\n",
    "            print(t[(e_j,f_i)], ibm1.translation_table[f_i][e_j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i 0  j 1  l_e 3  l_f 2\n",
      "i 1  j 0  l_e 3  l_f 2\n",
      "i 0  j 1  l_e 3  l_f 2\n",
      "i 1  j 0  l_e 3  l_f 2\n",
      "i 0  j 1  l_e 3  l_f 2\n",
      "i 1  j 0  l_e 3  l_f 2\n",
      "i 0  j 1  l_e 3  l_f 2\n",
      "i 1  j 0  l_e 3  l_f 2\n",
      "i 0  j 1  l_e 3  l_f 2\n",
      "i 1  j 0  l_e 3  l_f 2\n",
      "i 0  j 1  l_e 3  l_f 2\n",
      "i 1  j 0  l_e 3  l_f 2\n",
      "i 0  j 0  l_e 2  l_f 2\n",
      "i 0  j 0  l_e 2  l_f 2\n",
      "i 0  j 0  l_e 2  l_f 2\n",
      "------\n",
      "i 1  j 0  l_e 2  l_f 3\n",
      "i 1  j 0  l_e 2  l_f 3\n",
      "i 0  j 0  l_e 2  l_f 2\n",
      "i 1  j 0  l_e 2  l_f 3\n",
      "i 1  j 0  l_e 2  l_f 3\n",
      "i 0  j 0  l_e 2  l_f 2\n",
      "i 1  j 0  l_e 2  l_f 3\n",
      "i 1  j 0  l_e 2  l_f 3\n",
      "i 0  j 0  l_e 2  l_f 2\n"
     ]
    }
   ],
   "source": [
    "#learning translation probabilitity distributions from sentence-aligned parallel text\n",
    "#expectation maximization algorithm\n",
    "#Input: set of sentence pairs (e,f) \n",
    "#Output: translation prob. t (lexical translation) and a (alignment)\n",
    "#S.99 Figure 4.7\n",
    "#TODO e = [None] + e\n",
    "def EM_IBM_Model_2 (e_set,f_set):\n",
    "    #initialize t(e|f) with IBM Model 1\n",
    "    t = EM_IBM_Model_1(e_set,f_set)\n",
    "    #initialize a(i|j, l_e, l_f)\n",
    "    e_lengths = [len(e) for e in e_set]\n",
    "    f_lengths = [len(f) for f in f_set]\n",
    "    a = {}\n",
    "    for k in range (0, len(e_set)): #for every sentence\n",
    "        e = e_set[k]\n",
    "        f = f_set[k]\n",
    "        l_e = len(e)\n",
    "        l_f = len(f)\n",
    "        for j in range(0, l_e):\n",
    "            for i in range(0, l_f):\n",
    "                a[(i,j,l_e,l_f)] = 1/(l_f + 1)\n",
    "    #sets of distinct words\n",
    "    e_words = list(set([item for sublist in e_set for item in sublist]))\n",
    "    f_words = list(set([item for sublist in f_set for item in sublist]))\n",
    "    last_t = {} #to fail first comparison\n",
    "    step = 0\n",
    "    #iterate until convergence\n",
    "    while not (is_converged(t, last_t)) and step < max_steps:\n",
    "        #initialize\n",
    "        count = {(e,f): 0 for f in f_words for e in e_words}\n",
    "        total = {f: 0 for f in f_words}\n",
    "        total_a = {}\n",
    "        count_a = {}\n",
    "        for l_e in e_lengths:\n",
    "            for l_f in f_lengths:\n",
    "                for j in range(0,l_e):\n",
    "                    total_a[(j,l_e,l_f)] = 0\n",
    "                    for i in range(0,l_f):\n",
    "                        count_a[(i,j,l_e,l_f)] = 0\n",
    "        for k in range(0, len(e_set)):\n",
    "            e = e_set[k]\n",
    "            f = f_set[k]\n",
    "            l_e = len(e)\n",
    "            l_f = len(f)\n",
    "            #compute normalization\n",
    "            s_total = {}\n",
    "            for j in range(0, l_e):\n",
    "                e_j = e[j]\n",
    "                s_total[e_j] = 0\n",
    "                for i in range(0, l_f):\n",
    "                    f_i = f[i]\n",
    "                    s_total[e_j] += t[(e_j,f_i)] * a[(i,j,l_e,l_f)]\n",
    "            #collect counts\n",
    "            for j in range(0, l_e):\n",
    "                for i in range(0, l_f):\n",
    "                    e_j = e[j]\n",
    "                    f_i = f[i]\n",
    "                    c = t[(e_j,f_i)] * a[(i,j,l_e,l_f)] / s_total[e_j]\n",
    "                    count[(e_j,f_i)] += c\n",
    "                    total[f_i] += c\n",
    "                    count_a[(i,j,l_e,l_f)] += c\n",
    "                    total_a[(j,l_e,l_f)] += c\n",
    "        #estimate probabilities\n",
    "        last_t = copy.deepcopy(t)\n",
    "        for f_i in f_words:\n",
    "            for e_j in e_words:\n",
    "                if (e_j,f_i) in t:\n",
    "                    t[(e_j,f_i)] = count[(e_j,f_i)] / total[f_i]\n",
    "        for l_e in e_lengths:\n",
    "            for l_f in f_lengths:\n",
    "                for i in range(0,l_f):\n",
    "                    for j in range(0,l_e):\n",
    "                        if (i,j,l_e,l_f) in a:\n",
    "                            a[(i,j,l_e,l_f)] = count_a[(i,j,l_e,l_f)] / total_a[(j,l_e,l_f)]    \n",
    "        step += 1\n",
    "    return t, a\n",
    "\n",
    "#compare t and a table to IBM Model 2 implementation of nltk library\n",
    "def compare_to_nltk(t,a,src,tar):\n",
    "    import nltk\n",
    "    from nltk.translate import AlignedSent, IBMModel2\n",
    "    bitext = []\n",
    "    bitext.append(AlignedSent(['das', 'Haus'], ['house','the','the']))\n",
    "    bitext.append(AlignedSent(['das', 'Buch'], ['the', 'the','book']))\n",
    "    bitext.append(AlignedSent(['ein', 'Buch'], ['a', 'book']))\n",
    "    ibm2 = IBMModel2(bitext, 49)\n",
    "\n",
    "    for f_i in f_words:\n",
    "        for e_j in e_words:\n",
    "            if (e_j,f_i) in t:\n",
    "                bool = (ibm2.translation_table[f_i][e_j] > 0.7) == (t[(e_j,f_i)] > 0.7)\n",
    "                if bool == False: print('t', t[(e_j,f_i)], ibm2.translation_table[f_i][e_j])\n",
    "\n",
    "    e_lengths = [len(e) for e in src]\n",
    "    f_lengths = [len(f) for f in tar]\n",
    "    for l_e in e_lengths:\n",
    "        for l_f in f_lengths:\n",
    "            for i in range(0,l_f):\n",
    "                for j in range(0,l_e):\n",
    "                    if (i,j,l_e,l_f) in a:\n",
    "                        bool = (a[(i,j,l_e,l_f)] > 0.7) == (ibm2.alignment_table[j][i][l_f][l_e] > 0.7)\n",
    "                        if bool == False: \n",
    "                            #print('a', a[(i,j,l_e,l_f)], ibm2.alignment_table[j][i][l_f][l_e])\n",
    "                            print('i',i,' j',j,' l_e',l_e,' l_f',l_f)\n",
    "                        #else:\n",
    "                            #print('true','i',i,' j',j,' l_e',l_e,' l_f',l_f)\n",
    "#TODO my a is 1 when it should be low :(\n",
    "#TODO wrong for other direction\n",
    "\n",
    "t, a = EM_IBM_Model_2(e_train,f_train)\n",
    "compare_to_nltk(t,a,e_train,f_train)\n",
    "#print('t',t)\n",
    "#print('a',a)\n",
    "print('------')\n",
    "t, a = EM_IBM_Model_2(f_train,e_train)\n",
    "compare_to_nltk(t,a,f_train,e_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p( ['the', 'book'] | ['das', 'Buch'] ) = 0.0\n",
      "a: p( ['the', 'book'] | ['das', 'Buch'] ) = 0\n",
      "p( ['kgefw', 'fek'] | ['das', 'Buch'] ) = 0.0\n",
      "a: p( ['kgefw', 'fek'] | ['das', 'Buch'] ) = 0\n"
     ]
    }
   ],
   "source": [
    "#Testing\n",
    "\n",
    "#returns most likely aligned input word position i for a given position j in output sentence\n",
    "def a_fct(j,e,f,t):\n",
    "    e_j = e[j]\n",
    "    max_t = 0\n",
    "    max_i = None\n",
    "    for i in range(0,len(f)):\n",
    "        f_i = f[i]\n",
    "        if (e_j,f_i) in t and t[(e_j,f_i)] > max_t: \n",
    "            max_t = t[(e_j,f_i)]\n",
    "            max_i = i\n",
    "    return max_i\n",
    "    #TODO return max (a*t)!!!!\n",
    "\n",
    "#p(e|f) for IBM Model 1\n",
    "#S.90 Figure 4.10\n",
    "def prob_e_given_f_1 (e, f, epsilon, t):\n",
    "    prod = 1\n",
    "    for e_j in e: #TODO starts at 1 not at 0!?\n",
    "        sum = 0\n",
    "        for f_i in f:\n",
    "            if (e_j,f_i) in t:\n",
    "                sum += t[(e_j,f_i)]\n",
    "        prod *= sum\n",
    "    return ( epsilon / len(f)**len(e) ) * prod\n",
    "    #without +1 for NONE\n",
    "\n",
    "#p(e|f) for IBM Model 2\n",
    "#S.98 Figure 4.26\n",
    "#TODO we can leave out for i, cause i isnt used?\n",
    "#TODO but maybe we cant cause the sum changes eventhough!\n",
    "def prob_e_given_f_2 (e, f, epsilon, t, a):\n",
    "    l_e = len(e)\n",
    "    l_f = len(f)\n",
    "    prod = 1\n",
    "    for j in range(0, l_e): #TODO starts at 1 not at 0!\n",
    "        e_j = e[j]\n",
    "        sum = 0\n",
    "        for i in range(0, l_f): \n",
    "            f_i = f[i]\n",
    "            if (e_j,f_i) in t:\n",
    "                f_aj = f[a_fct(j,e,f,t)]\n",
    "                sum += t[(e_j,f_aj)]*a[(a_fct(j,e,f,t),j,l_e,l_f)]\n",
    "        prod *= sum\n",
    "    return epsilon * prod\n",
    "#when removing += it's 1 and not 4 anymore which is good - so maybe dont sum over i?\n",
    "\n",
    "for i in range(0,len(f_test)):\n",
    "    f = f_test[i]\n",
    "    e = e_test[i]\n",
    "    #arg max_e p(e)p(f|e) decoding manually #TODO how???\n",
    "    #calculate p(e|f)\n",
    "    #print(e, prob_e_given_f(e, f, 0.001, t))\n",
    "    \n",
    "f = ['das','Buch']\n",
    "e = ['the','book']\n",
    "print('p(',e,'|',f,') =',prob_e_given_f_1(e, f, 1, t))\n",
    "print('a: p(',e,'|',f,') =',prob_e_given_f_2(e, f, 1, t, a))\n",
    "e = ['kgefw','fek']\n",
    "print('p(',e,'|',f,') =',prob_e_given_f_1(e, f, 1, t))\n",
    "print('a: p(',e,'|',f,') =',prob_e_given_f_2(e, f, 1, t, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['house', 'the', 'the'] ['das', 'Haus']\n",
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [0. 0.]]\n",
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "['the', 'the', 'book'] ['das', 'Buch']\n",
      "[[1. 0.]\n",
      " [0. 0.]\n",
      " [0. 1.]]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "['a', 'book'] ['ein', 'Buch']\n",
      "[[1. 0.]\n",
      " [0. 1.]]\n",
      "[[1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#Phrase-based model\n",
    "\n",
    "#Sentence Alignments in both directions with Viterbi algorithm using IBM Model 2\n",
    "#While it is true that during the EM training fractional counts are\n",
    "#collected over a probability distribution of possible alignments, one\n",
    "#word alignment sticks out: the most probable alignment, also called the\n",
    "#Viterbi alignment\n",
    "#> output the Viterbi alignment of the last iteration\n",
    "#> run IBM model training in both directions\n",
    "\n",
    "#Decoding\n",
    "\n",
    "#TODO I have to use ibm model 2!!!!\n",
    "#Viterbi alignment\n",
    "#find the most probable alignment for a given translation pair\n",
    "#f2e = ibmmodel2.viterbi_alignment(es, fs, *f2e_train).items()\n",
    "#e2f = ibmmodel2.viterbi_alignment(fs, es, *e2f_train).items()\n",
    "#align = alignment(es, fs, e2f, f2e)  # symmetrized alignment\n",
    "\"\"\"\"\"\n",
    "def viterbi_alignment(es, fs, t, a):\n",
    "    '''\n",
    "    return\n",
    "        dictionary\n",
    "            e in es -> f in fs\n",
    "    '''\n",
    "    max_a = collections.defaultdict(float)\n",
    "    l_e = len(es)\n",
    "    l_f = len(fs)\n",
    "    for (j, e) in enumerate(es, 1):\n",
    "        current_max = (0, -1)\n",
    "        for (i, f) in enumerate(fs, 1):\n",
    "            val = t[(e, f)] * a[(i, j, l_e, l_f)]\n",
    "            # select the first one among the maximum candidates\n",
    "            if current_max[1] < val:\n",
    "                current_max = (i, val)\n",
    "        max_a[j] = current_max[0]\n",
    "    return max_a\n",
    "\"\"\"\"\"\n",
    "#a_hat = arg max_a p(e_j,f_i)\n",
    "def align(e_set, f_set, t):\n",
    "    all_aligned = {}\n",
    "    for sentence in range(0,len(e_set)):\n",
    "        e = e_set[sentence]\n",
    "        f = f_set[sentence]\n",
    "        aligned = np.zeros((len(e),len(f)))\n",
    "        for i in range(0, len(f)): \n",
    "            f_i = f[i]\n",
    "            best_prob = 0\n",
    "            best_j = 0\n",
    "            for j in range(0, len(e)):\n",
    "                e_j = e[j]\n",
    "                if (e_j,f_i) in t and t[(e_j,f_i)] > best_prob:\n",
    "                    best_prob = t[(e_j,f_i)]\n",
    "                    best_j = j\n",
    "            aligned[best_j][i] = 1\n",
    "        all_aligned[sentence] = aligned\n",
    "    return all_aligned\n",
    "\n",
    "#IBM Models for word alignment\n",
    "t_e2f = EM_IBM_Model_1(e_train,f_train)\n",
    "all_aligned_e2f = align(e_train,f_train,t_e2f)\n",
    "t_f2e = EM_IBM_Model_1(f_train,e_train)\n",
    "all_aligned_f2e = align(f_train,e_train,t_f2e)\n",
    "#transpose so that alignments are same shape\n",
    "all_aligned_f2e = [np.transpose(f2e) for f2e in list(all_aligned_f2e.values())]\n",
    "for sentence in range(0,len(e_train)):\n",
    "    print(e_train[sentence], f_train[sentence])\n",
    "    print(all_aligned_e2f[sentence])\n",
    "    print(all_aligned_f2e[sentence])\n",
    "    #x foreign, y English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]]\n",
      "[[1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#Symmetrization of alignment\n",
    "#for every sentence a alignment matrix esentence - fsentence\n",
    "#0 if not aligned, 1 if aligned \n",
    "#2 directions -> make intersection\n",
    "#make union, neighbour logic\n",
    "#for each sentence combine the two alignments using the word alignment algorithm\n",
    "#>merged by taking the intersection or the union of alignment points of each alignment\n",
    "#The algorithm starts with the intersection of the alignments. In\n",
    "#the growing step, neighboring alignment points that are in the union but\n",
    "#not in the intersection are added. In the final step, alignment points for\n",
    "#words that are still unaligned are added.\n",
    "\n",
    "def intersect(a,b):\n",
    "    intersect = np.zeros(np.shape(a))\n",
    "    for j in range(0,np.shape(a)[0]):\n",
    "        for i in range(0,np.shape(a)[1]):\n",
    "            intersect[j][i] = 1 if (a[j][i]==1 and b[j][i]==1) else 0\n",
    "    return intersect\n",
    "\n",
    "def union(a,b):\n",
    "    union = np.zeros(np.shape(a))\n",
    "    for j in range(0,np.shape(a)[0]):\n",
    "        for i in range(0,np.shape(a)[1]):\n",
    "            union[j][i] = 1 if (a[j][i]==1 or b[j][i]==1) else 0\n",
    "    return union\n",
    "\n",
    "def aligned_e(j,a):\n",
    "    return np.sum(a[j]) != 0\n",
    "\n",
    "def aligned_f(i,a):\n",
    "    return np.sum([a[j][i] for j in range(0,np.shape(a)[0])]) != 0\n",
    "\n",
    "def aligned(j, i, a):\n",
    "    return a[j][i] == 1\n",
    "\n",
    "def neighbour_point(j, i):\n",
    "    neighbouring = [(-1,0),(0,-1),(1,0),(0,1),(-1,-1),(-1,1),(1,-1),(1,1)]\n",
    "    return [(a+j, b+i) for (a,b) in neighbouring]\n",
    "\n",
    "def grow_diag_final(e2f, f2e):\n",
    "    a = intersect(e2f,f2e)\n",
    "    union_alignment = union(e2f,f2e)\n",
    "    last_a = []\n",
    "    #iterate until convergence\n",
    "    while not np.array_equal(last_a,a):\n",
    "        last_a = copy.deepcopy(a)\n",
    "        for j in range(0,np.shape(e2f)[0]):\n",
    "            for i in range(0,np.shape(e2f)[1]):\n",
    "                if aligned(j, i, a):\n",
    "                    for (j_new, i_new) in neighbour_point(j, i):\n",
    "                        if j_new in a and i_new in a[j_new]:\n",
    "                            if (not aligned_e(j_new, a) or not aligned_f(i_new, a)) and (j_new, i_new) in union_alignment:\n",
    "                                a[j_new][i_new] = 1\n",
    "    for j_new in range(0,np.shape(a)[0]):\n",
    "        for i_new in range(0,np.shape(a)[1]):\n",
    "            if (not aligned_e(j_new,a) or not aligned_f(i_new,a)) and (j_new, i_new) in union_alignment: \n",
    "                a[j_new][i_new] = 1\n",
    "    return a\n",
    "\n",
    "#call for every sentence pair\n",
    "all_alignments = {}\n",
    "for index in range(0,len(e_train)):\n",
    "    e2f = all_aligned_e2f[index]\n",
    "    f2e = all_aligned_f2e[index]\n",
    "    alignment = grow_diag_final(e2f, f2e)\n",
    "    print(alignment)\n",
    "    all_alignments[index] = alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(['house'], ['Haus']), (['house', 'the', 'the'], ['das', 'Haus']), (['the', 'the'], ['das'])], [(['the', 'the'], ['das']), (['the', 'the', 'book'], ['das', 'Buch']), (['book'], ['Buch'])], [(['a'], ['ein']), (['a', 'book'], ['ein', 'Buch']), (['book'], ['Buch'])]]\n"
     ]
    }
   ],
   "source": [
    "#Phrase extraction from symmetrized alignment table\n",
    "#phrase extraction algorithm Use a suitable threshold for maximum phrase length and \n",
    "#estimate the phrase translation probabilities. \n",
    "#Phrase extraction algorithm: \n",
    "#For each English phrase estart .. eend,\n",
    "#the minimal phrase of aligned foreign words is identified fstart .. fend. Words in\n",
    "#the foreign phrase are not allowed to be aligned with English words outside the\n",
    "#English phrase. This pair of phrases is added, along with additional phrase pairs\n",
    "#that include additional unaligned foreign words at the edge of the foreign phrase.\n",
    "#Input: word alignment A for sentence pair (e,f)\n",
    "#Output: set of phrase pairs BP\n",
    "\n",
    "#TODO max phrase length\n",
    "def extract(f, f_start, f_end, e, e_start, e_end, a):\n",
    "    #check if at least one alignment point\n",
    "    if f_end < 0:\n",
    "        return []\n",
    "    #check if alignment points violate consistency\n",
    "    for j in range(0, np.shape(a)[0]):\n",
    "        for i in range(0, np.shape(a)[1]):\n",
    "            if a[j][i] == 1:\n",
    "                if (f_start <= i <= f_end) and (j < e_start or j > e_end):\n",
    "                    return []\n",
    "    #add pharse pairs (incl. additional unaligned f)\n",
    "    E = []\n",
    "    f_s = f_start\n",
    "    first1 = True\n",
    "    while not aligned_f(f_s,a) or first1:\n",
    "        first1 = False\n",
    "        f_e = f_end\n",
    "        first2 = True\n",
    "        while not aligned_f(f_e,a) or first2:\n",
    "            first2 = False\n",
    "            E.append((e[e_start:e_end+1], f[f_s:f_e+1]))\n",
    "            f_e += 1\n",
    "            if f_e >= np.shape(a)[1]: break\n",
    "        f_s -= 1\n",
    "        if f_s < 0: break\n",
    "    return E\n",
    "\n",
    "e_train = [['house','the','the'],['the','the','book'],['a','book']]\n",
    "f_train = [['das','Haus'],['das','Buch'],['ein','Buch']]\n",
    "\"\"\"\"\"\n",
    "e_train = [['michael','assumes','that','he','will','stay','in','the','house']]\n",
    "f_train = [['michael','geht','davon','aus',',','dass','er','im','haus','bleibt']]\n",
    "all_alignments = {0:[[1,0,0,0,0,0,0,0,0,0],\n",
    "                   [0,1,1,1,0,0,0,0,0,0],\n",
    "                   [0,0,0,0,0,1,0,0,0,0],\n",
    "                   [0,0,0,0,0,0,1,0,0,0],\n",
    "                   [0,0,0,0,0,0,0,0,0,1],\n",
    "                   [0,0,0,0,0,0,0,0,0,1],\n",
    "                   [0,0,0,0,0,0,0,1,0,0],\n",
    "                   [0,0,0,0,0,0,0,1,0,0],\n",
    "                   [0,0,0,0,0,0,0,0,1,0]]}\n",
    "\"\"\"\"\"\n",
    "\n",
    "def phrase_extraction(e_set,f_set, all_alignments):\n",
    "    all_BP = []\n",
    "    for index in all_alignments:\n",
    "        alignment = all_alignments[index]\n",
    "        e = e_set[index]\n",
    "        f = f_set[index]\n",
    "        BP = []\n",
    "        for e_start in range(0,len(e)):\n",
    "            for e_end in range(e_start,len(e)):\n",
    "                #find the minimally matching foreign phrase\n",
    "                (f_start, f_end) = (len(f)-1, -1)\n",
    "                for j in range(0, np.shape(alignment)[0]):\n",
    "                    for i in range(0, np.shape(alignment)[1]):\n",
    "                        if alignment[j][i] == 1:\n",
    "                            if e_start <= j <= e_end:\n",
    "                                f_start = min(i, f_start)\n",
    "                                f_end = max(i, f_end)\n",
    "                BP.append(extract(f, f_start, f_end, e, e_start, e_end, alignment))\n",
    "        all_BP.append([item for sublist in BP for item in sublist])\n",
    "    return all_BP\n",
    "\n",
    "all_BP = phrase_extraction(e_train, f_train, all_alignments)\n",
    "#for BP in all_BP:\n",
    "print(all_BP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"(['house'], ['Haus'])\": 1, \"(['house', 'the', 'the'], ['das', 'Haus'])\": 1, \"(['the', 'the'], ['das'])\": 2, \"(['the', 'the', 'book'], ['das', 'Buch'])\": 1, \"(['book'], ['Buch'])\": 2, \"(['a'], ['ein'])\": 1, \"(['a', 'book'], ['ein', 'Buch'])\": 1}\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#probabilistic phrase translation table\n",
    "#For each sentence pair, we extract a number of phrase pairs.\n",
    "#Then, we count in how many sentence pairs a particular phrase pair is\n",
    "#extracted and store this number in count(e,f).\n",
    "all_BP_flat = [item for sublist in all_BP for item in sublist]\n",
    "count={}\n",
    "for (a,b) in all_BP_flat:\n",
    "    #key = (frozenset(a),frozenset(b))\n",
    "    key = str((a,b))\n",
    "    count[key] = count.get(key,0) + 1\n",
    "print(count)\n",
    "\n",
    "#Scoring Phrase Translations\n",
    "#the translation probability φ(f|e) is estimated by the relative frequency\n",
    "def φ(e, f):\n",
    "    key = str((e,f))\n",
    "    sum = 0\n",
    "    for f_i in f:\n",
    "        if str((e,[f_i])) in count:\n",
    "            sum +=count[str((e,[f_i]))]\n",
    "    if str((e,f)) in count:\n",
    "        return count[key] / sum\n",
    "    else: return 0\n",
    "\n",
    "print(φ(['house'],['Haus']))\n",
    "#print(φ(['the','house'],['das','Haus']))\n",
    "#TODO translation probability is not working for phrases containing several words\n",
    "#maybe other data structure ['the','house']:[['das','Haus'],['ein','Haus']]\n",
    "#faster sometimes but slower if translation in other direction\n",
    "\n",
    "#TODO For the estimation of the phrase translation probabilities, not all\n",
    "#phrase pairs have to be loaded into memory. It is possible to efficiently\n",
    "#estimate the probability distribution by storing and sorting the extracted\n",
    "#phrases on disk. Similarly, when using the translation table for the translation of a single sentence, only a small fraction of it is needed and may\n",
    "#be loaded on demand.\n",
    "#think about saving stuff in file\n",
    "#• Phrase translation table typically bigger than corpus\n",
    "#... even with limits on phrase lengths (e.g., max 7 words)\n",
    "#→ Too big to store in memory?\n",
    "#• Solution for training\n",
    "#– extract to disk, sort, construct for one source phrase at a time\n",
    "#• Solutions for decoding\n",
    "#– on-disk data structures with index for quick look-ups\n",
    "#– suffix arrays to create phrase pairs on demand\n",
    "#TODO use numpy for everything\n",
    "\n",
    "#phrase-based statistical machine translation model\n",
    "#• the phrase translation table φ(f |e);\n",
    "#• the reordering model d;\n",
    "#• the language model pLM(e).\n",
    "#e_best = argmax_e prod_i φ(f|e) * d(start_i - end_i_min_1 - 1) * prod_i PLM(e_i|e_i_min_1)\n",
    "\n",
    "#distance based reordering model d\n",
    "#This model gives a cost linear to the reordering distance. For instance, skipping over two words costs twice as much as skipping over one word.\n",
    "#<source-phrase> & <target-phrase> : <weight-1> <weight-2> ... <weight-k> for k of 6 / 8\n",
    "def d(x):\n",
    "    return 0#alpha**|x| #TODO i dont get it\n",
    "\n",
    "#bigram language model #TODO i dont get it\n",
    "#<probability> <word-1><word-2>  <back-off-weight>\n",
    "#eg -0.2359 When will 0.1011\n",
    "#test the model using the test data. \n",
    "#for a source sentence, you can generate some example target sentences (word sequences) \n",
    "#and phrases “manually”; i.e. you will do the decoding process manually. \n",
    "#That is, for a source sentence, generate a possible target sentence,\n",
    "#divide both sentences into phrases as you wish, and assume that you know the phrase\n",
    "#correspondences. Then, calculate the probability of the target sentence given the source\n",
    "#sentence using the p(f|e)*p LM (e) equation (i.e. using the standard model with three\n",
    "#components). \n",
    "\n",
    "#You can train a simple bigram language model and a reordering model.\n",
    "\n",
    "#Translation model - provides phrases in the source language with learned possible target language \n",
    "#translations and the probabilities thereof.\n",
    "#Reordering model - stores information about probable translation orders of the phrases within the \n",
    "#source text, based on the observed source and target phrases and their alignments.\n",
    "#Language model - reflects the likelihood of this or that phrase in the target language to occur. \n",
    "#In other words, it is used to evaluate the obtained translation for being \"sound\" in the target language.\n",
    "#Note that, the language model is typically learned from a different corpus in a target language.\n",
    "\n",
    "#With these three models at hand one can perform decoding, which is a synonym to a translation process. \n",
    "#SMT decoding is performed by exploring the state space of all possible translations and reordering of the \n",
    "#source language phrases within one sentence. The purpose of decoding, as indicated by the maximization \n",
    "#procedure at the bottom of the figure above, is to find a translation with the largest possible probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QUESTIONS\n",
    "\n",
    "#IBM Model 2: Do I calculate a correctly?\n",
    "#> compare with other implementations\n",
    "#> NOOOOO a is wrong :/\n",
    "\n",
    "#p(e|f) for IBM Model 2: Gives result 4 should be 1?\n",
    "#> compare with other implementations\n",
    "\n",
    "#TODO did I have to implement all that? \n",
    "#what exactly do i have to do? first train, then do what with t?\n",
    "#decoding = calculating max p(e|f)?\n",
    "\n",
    "#how to output the Viterbi alignment of the last iteration IBM 2?\n",
    "\n",
    "#how to implement distance based reordering model?\n",
    "\n",
    "#how to implement bigram language model?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
