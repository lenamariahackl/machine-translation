{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "#read in corpora\n",
    "used_nr_corpus_sentences = 30 #688670 lines\n",
    "corpus_path = \"./Paralel Corpus/\"\n",
    "filename_e = \"BU_en.txt\"\n",
    "filename_f = \"BU_tr.txt\"\n",
    "\n",
    "e_file = open(corpus_path + filename_e, \"r\", encoding='utf-8-sig')\n",
    "e_lines = e_file.readlines()\n",
    "f_file = open(corpus_path + filename_f, \"r\", encoding='utf-8-sig')\n",
    "f_lines = f_file.readlines()\n",
    "\n",
    "def tokenize(lines):\n",
    "    lines = [line.rstrip() for line in lines[:used_nr_corpus_sentences]] #remove \\n\n",
    "    words = [line.split() for line in lines] #split by words\n",
    "    unpunctuated = str.maketrans('', '', string.punctuation) #remove punctuation & make Lowercase\n",
    "    lower = [[word.translate(unpunctuated).lower() for word in sentence] for sentence in words]\n",
    "    return lower\n",
    "\n",
    "f = tokenize(f_lines)\n",
    "e = tokenize(e_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "max_steps = 20\n",
    "training_split = 0.9\n",
    "\n",
    "def split_dataset(dataset, training_split):\n",
    "    np.random.shuffle(dataset)\n",
    "    nr_train_samples = int(training_split*len(dataset))\n",
    "    return dataset[:nr_train_samples], dataset[nr_train_samples:]\n",
    "\n",
    "e_train, e_test = split_dataset(e, training_split)\n",
    "f_train, f_test = split_dataset(f, training_split)\n",
    "e_train = [['the','house'],['the','book'],['a','book']]\n",
    "f_train = [['das','Haus'],['das','Buch'],['ein','Buch']]\n",
    "\n",
    "e_words = set([item for sublist in e_train for item in sublist])\n",
    "f_words = set([item for sublist in f_train for item in sublist])\n",
    "    \n",
    "#test if the two sets are equivalent\n",
    "def is_converged(t, last_t):\n",
    "    if ( t == last_t): print( t, last_t)\n",
    "    return t == last_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'house'], ['the', 'book'], ['a', 'book']]\n",
      "{('the', 'das'): 0.25, ('the', 'Haus'): 0.25, ('house', 'das'): 0.25, ('house', 'Haus'): 0.25, ('the', 'Buch'): 0.25, ('book', 'das'): 0.25, ('book', 'Buch'): 0.25, ('a', 'ein'): 0.25, ('a', 'Buch'): 0.25, ('book', 'ein'): 0.25}\n",
      "---\n",
      "{('the', 'das'): 0.5, ('the', 'Haus'): 0.5, ('house', 'das'): 0.25, ('house', 'Haus'): 0.5, ('the', 'Buch'): 0.25, ('book', 'das'): 0.25, ('book', 'Buch'): 0.5, ('a', 'ein'): 0.5, ('a', 'Buch'): 0.25, ('book', 'ein'): 0.5}\n",
      "---\n",
      "{('the', 'das'): 0.6363636363636364, ('the', 'Haus'): 0.4285714285714286, ('house', 'das'): 0.18181818181818182, ('house', 'Haus'): 0.5714285714285715, ('the', 'Buch'): 0.18181818181818182, ('book', 'das'): 0.18181818181818182, ('book', 'Buch'): 0.6363636363636364, ('a', 'ein'): 0.5714285714285715, ('a', 'Buch'): 0.18181818181818182, ('book', 'ein'): 0.4285714285714286}\n",
      "---\n",
      "{('the', 'das'): 0.7478974515333995, ('the', 'Haus'): 0.34661354581673304, ('house', 'das'): 0.13126000457351933, ('house', 'Haus'): 0.6533864541832669, ('the', 'Buch'): 0.1208425438930813, ('book', 'das'): 0.1208425438930813, ('book', 'Buch'): 0.7478974515333995, ('a', 'ein'): 0.6533864541832669, ('a', 'Buch'): 0.13126000457351933, ('book', 'ein'): 0.34661354581673304}\n",
      "---\n",
      "{('the', 'das'): 0.8344395715124338, ('the', 'Haus'): 0.2755211789521138, ('house', 'das'): 0.09039519669722251, ('house', 'Haus'): 0.7244788210478862, ('the', 'Buch'): 0.07516523179034365, ('book', 'das'): 0.07516523179034365, ('book', 'Buch'): 0.8344395715124339, ('a', 'ein'): 0.7244788210478862, ('a', 'Buch'): 0.09039519669722253, ('book', 'ein'): 0.2755211789521138}\n",
      "---\n",
      "{('the', 'das'): 0.8960831177730378, ('the', 'Haus'): 0.21826012818351043, ('house', 'das'): 0.05955396752014332, ('house', 'Haus'): 0.7817398718164896, ('the', 'Buch'): 0.044362914706818864, ('book', 'das'): 0.04436291470681886, ('book', 'Buch'): 0.8960831177730377, ('a', 'ein'): 0.7817398718164896, ('a', 'Buch'): 0.059553967520143324, ('book', 'ein'): 0.21826012818351037}\n",
      "---\n",
      "{('the', 'das'): 0.9370850546370292, ('the', 'Haus'): 0.1740899223359456, ('house', 'das'): 0.03775541775818424, ('house', 'Haus'): 0.8259100776640544, ('the', 'Buch'): 0.025159527604786774, ('book', 'das'): 0.025159527604786777, ('book', 'Buch'): 0.9370850546370291, ('a', 'ein'): 0.8259100776640544, ('a', 'Buch'): 0.037755417758184244, ('book', 'ein'): 0.17408992233594558}\n",
      "---\n",
      "{('the', 'das'): 0.9629780468299598, ('the', 'Haus'): 0.14077092021669385, ('house', 'das'): 0.023166041077433423, ('house', 'Haus'): 0.8592290797833062, ('the', 'Buch'): 0.01385591209260682, ('book', 'das'): 0.013855912092606825, ('book', 'Buch'): 0.9629780468299598, ('a', 'ein'): 0.8592290797833062, ('a', 'Buch'): 0.023166041077433423, ('book', 'ein'): 0.14077092021669385}\n",
      "---\n",
      "{('the', 'das'): 0.9787023808243721, ('the', 'Haus'): 0.11580912945722344, ('house', 'das'): 0.013827033383395878, ('house', 'Haus'): 0.8841908705427766, ('the', 'Buch'): 0.007470585792232024, ('book', 'das'): 0.0074705857922320274, ('book', 'Buch'): 0.9787023808243721, ('a', 'ein'): 0.8841908705427766, ('a', 'Buch'): 0.013827033383395878, ('book', 'ein'): 0.11580912945722344}\n",
      "---\n",
      "{('the', 'das'): 0.9879698622753386, ('the', 'Haus'): 0.09703579479955045, ('house', 'das'): 0.008063141240695343, ('house', 'Haus'): 0.9029642052004496, ('the', 'Buch'): 0.0039669964839660855, ('book', 'das'): 0.003966996483966088, ('book', 'Buch'): 0.9879698622753386, ('a', 'ein'): 0.9029642052004496, ('a', 'Buch'): 0.008063141240695343, ('book', 'ein'): 0.09703579479955045}\n",
      "---\n",
      "{('the', 'das'): 0.9933053397165424, ('the', 'Haus'): 0.08276408100718743, ('house', 'das'): 0.0046110887522225865, ('house', 'Haus'): 0.9172359189928124, ('the', 'Buch'): 0.0020835715312350995, ('book', 'das'): 0.0020835715312351013, ('book', 'Buch'): 0.9933053397165423, ('a', 'ein'): 0.9172359189928124, ('a', 'Buch'): 0.004611088752222586, ('book', 'ein'): 0.08276408100718743}\n",
      "---\n",
      "{('the', 'das'): 0.9963200684152884, ('the', 'Haus'): 0.07175344394156298, ('house', 'das'): 0.002594284648342067, ('house', 'Haus'): 0.928246556058437, ('the', 'Buch'): 0.0010856469363694668, ('book', 'das'): 0.0010856469363694676, ('book', 'Buch'): 0.9963200684152885, ('a', 'ein'): 0.928246556058437, ('a', 'Buch'): 0.0025942846483420668, ('book', 'ein'): 0.07175344394156298}\n",
      "---\n",
      "{('the', 'das'): 0.9979977832716698, ('the', 'Haus'): 0.06311600593235389, ('house', 'das'): 0.0014398758914981288, ('house', 'Haus'): 0.936883994067646, ('the', 'Buch'): 0.0005623408368321092, ('book', 'das'): 0.0005623408368321096, ('book', 'Buch'): 0.9979977832716698, ('a', 'ein'): 0.936883994067646, ('a', 'Buch'): 0.0014398758914981283, ('book', 'ein'): 0.06311600593235389}\n",
      "---\n",
      "{('the', 'das'): 0.9989198697821495, ('the', 'Haus'): 0.05622298399761185, ('house', 'das'): 0.000790152793637741, ('house', 'Haus'): 0.9437770160023881, ('the', 'Buch'): 0.00028997742421277495, ('book', 'das'): 0.00028997742421277517, ('book', 'Buch'): 0.9989198697821494, ('a', 'ein'): 0.9437770160023881, ('a', 'Buch'): 0.0007901527936377407, ('book', 'ein'): 0.05622298399761185}\n",
      "---\n",
      "{('the', 'das'): 0.9994214631956893, ('the', 'Haus'): 0.05062929787343242, ('house', 'das'): 0.0004295257586158875, ('house', 'Haus'): 0.9493707021265675, ('the', 'Buch'): 0.00014901104569497127, ('book', 'das'): 0.0001490110456949714, ('book', 'Buch'): 0.9994214631956893, ('a', 'ein'): 0.9493707021265675, ('a', 'Buch'): 0.0004295257586158874, ('book', 'ein'): 0.05062929787343242}\n",
      "---\n",
      "{('the', 'das'): 0.9996919929266808, ('the', 'Haus'): 0.04601805009203235, ('house', 'das'): 0.0002316458835994153, ('house', 'Haus'): 0.9539819499079677, ('the', 'Buch'): 7.636118971985197e-05, ('book', 'das'): 7.636118971985205e-05, ('book', 'Buch'): 0.9996919929266807, ('a', 'ein'): 0.9539819499079677, ('a', 'Buch'): 0.00023164588359941523, ('book', 'ein'): 0.04601805009203235}\n",
      "---\n",
      "{('the', 'das'): 0.9998368602354943, ('the', 'Haus'): 0.04216137092129015, ('house', 'das'): 0.00012409597183682286, ('house', 'Haus'): 0.9578386290787099, ('the', 'Buch'): 3.9043792668930235e-05, ('book', 'das'): 3.904379266893028e-05, ('book', 'Buch'): 0.9998368602354942, ('a', 'ein'): 0.9578386290787099, ('a', 'Buch'): 0.0001240959718368228, ('book', 'ein'): 0.04216137092129016}\n",
      "---\n",
      "{('the', 'das'): 0.9999139700059068, ('the', 'Haus'): 0.038893370263705374, ('house', 'das'): 6.610383909194634e-05, ('house', 'Haus'): 0.9611066297362946, ('the', 'Buch'): 1.9926155001251443e-05, ('book', 'das'): 1.9926155001251467e-05, ('book', 'Buch'): 0.999913970005907, ('a', 'ein'): 0.9611066297362946, ('a', 'Buch'): 6.610383909194632e-05, ('book', 'ein'): 0.03889337026370539}\n",
      "---\n",
      "{('the', 'das'): 0.9999548046766328, ('the', 'Haus'): 0.03609160537751424, ('house', 'das'): 3.5041861497632454e-05, ('house', 'Haus'): 0.9639083946224857, ('the', 'Buch'): 1.0153461869576255e-05, ('book', 'das'): 1.0153461869576265e-05, ('book', 'Buch'): 0.9999548046766328, ('a', 'ein'): 0.9639083946224857, ('a', 'Buch'): 3.504186149763244e-05, ('book', 'ein'): 0.03609160537751424}\n",
      "---\n",
      "{('the', 'das'): 0.9999763350200317, ('the', 'Haus'): 0.03366439092703859, ('house', 'das'): 1.849816981103972e-05, ('house', 'Haus'): 0.9663356090729615, ('the', 'Buch'): 5.166810157208551e-06, ('book', 'das'): 5.166810157208556e-06, ('book', 'Buch'): 0.9999763350200317, ('a', 'ein'): 0.9663356090729615, ('a', 'Buch'): 1.8498169811039712e-05, ('book', 'ein'): 0.03366439092703859}\n",
      "---\n",
      "{('the', 'das'): 0.9999876443477212, ('the', 'Haus'): 0.031542072170447605, ('house', 'das'): 9.729458618407095e-06, ('house', 'Haus'): 0.9684579278295524, ('the', 'Buch'): 2.6261936603029755e-06, ('book', 'das'): 2.6261936603029776e-06, ('book', 'Buch'): 0.9999876443477214, ('a', 'ein'): 0.9684579278295524, ('a', 'Buch'): 9.729458618407092e-06, ('book', 'ein'): 0.031542072170447605}\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "#learning translation probabilitity distributions from sentence-aligned parallel text\n",
    "#expectation maximization algorithm\n",
    "#Input: set of sentence pairs (e,f) \n",
    "#Output: translation prob. t(e|f)\n",
    "def EM_IBM_Model_1 (e_set,f_set):\n",
    "    #sets of distinct words\n",
    "    e_words = set([item for sublist in e_set for item in sublist])\n",
    "    f_words = set([item for sublist in f_set for item in sublist])\n",
    "    #initialize t(e|f) with uniform distribution\n",
    "    t = {}#{key: {key2: 1/len(e_words) for key2 in f_words} for key in e_words}\n",
    "    for k in range (0, len(e_set)):\n",
    "        e = e_set[k]\n",
    "        f = f_set[k]\n",
    "        for e_j in e:\n",
    "            for f_i in f:\n",
    "                t[(e_j,f_i)] = t.get((e_j,f_i),1/len(e_words))    \n",
    "    last_t = {0:1} #to fail first comparison\n",
    "    step = 0\n",
    "    #iterate until convergence\n",
    "    while not(is_converged(t, last_t)) and step < max_steps:\n",
    "        #initialize\n",
    "        count = {e: {f: 0 for f in f_words} for e in e_words}\n",
    "        total = {f: 0 for f in f_words}\n",
    "        for k in range (0, len(e_set)):\n",
    "            e = e_set[k]\n",
    "            f = f_set[k]\n",
    "            #compute normalization\n",
    "            #s_total = {key: 0 for key in e_words}\n",
    "            s_total = {}\n",
    "            for e_j in e:\n",
    "                s_total[e_j] = 0\n",
    "                for f_i in f:\n",
    "                    s_total[e_j] += t[(e_j,f_i)]\n",
    "            #collect counts\n",
    "            for f_i in f:\n",
    "                for e_j in e:\n",
    "                    count[e_j][f_i] += t[(e_j,f_i)] / s_total[e_j]\n",
    "                    total[f_i] += t[(e_j,f_i)] / s_total[e_j]\n",
    "        #estimate probabilities\n",
    "        last_t = copy.deepcopy(t)\n",
    "        for f_i in f_words:\n",
    "            for e_j in e_words:\n",
    "                if (e_j,f_i) in t:\n",
    "                    t[(e_j,f_i)] = count[e_j][f_i] / total[f_i] \n",
    "        step += 1\n",
    "        print(last_t)\n",
    "        print('---')\n",
    "        #print('count ',count)\n",
    "        #print('stotal ',s_total)\n",
    "        #print('total',total)\n",
    "        #print('last_t',last_t)\n",
    "    return t\n",
    "print(e_train)\n",
    "t = EM_IBM_Model_1(e_train,f_train)\n",
    "print(t)\n",
    "#how to iterate lengths to initialize a,\n",
    "#is sum over t ok?, why wrong values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': {'ein': 0.25, 'das': 1.125, 'Buch': 0.125, 'Haus': 0.25}, 'house': {'ein': 0.25, 'das': 0.125, 'Buch': 0.125, 'Haus': 1.25}, 'book': {'ein': 0.25, 'das': 0.125, 'Buch': 1.125, 'Haus': 0.25}, 'a': {'ein': 1.25, 'das': 0.125, 'Buch': 0.125, 'Haus': 0.25}}\n"
     ]
    }
   ],
   "source": [
    "#learning translation probabilitity distributions from sentence-aligned parallel text\n",
    "#expectation maximization algorithm\n",
    "#Input: set of sentence pairs (e,f) \n",
    "#Output: translation prob. t (lexical translation) and a (alignment)\n",
    "def EM_IBM_Model_2 (e_set,f_set):\n",
    "#TODO jump parameterisation for better performance???\n",
    "    #initialize t(e|f) with IBM Model 1\n",
    "    t = EM_IBM_Model_1(e_set,f_set)\n",
    "    #initialize a(i|j, l_e, l_f)\n",
    "    e_lengths = [len(e) for e in e_set]\n",
    "    f_lengths = [len(f) for f in f_set]\n",
    "    for l_e in e_lengths:\n",
    "        for l_f in f_lengths:\n",
    "            for i in range(0,l_f):\n",
    "                for j in range(0,l_e):\n",
    "                    a[(i,j,l_e,l_f)] = 1/(l_f + 1)\n",
    "    #sets of distinct words\n",
    "    e_words = list(set([item for sublist in e_set for item in sublist]))\n",
    "    f_words = list(set([item for sublist in f_set for item in sublist]))\n",
    "    last_t = {} #to fail first comparison\n",
    "    step = 0\n",
    "    #iterate until convergence\n",
    "    while ((not is_converged(t, last_t)) | step < max_steps):\n",
    "        #initialize\n",
    "        count = {e: {f: 1/len(e_words) for f in f_words} for e in e_words}\n",
    "        total = {f: 0 for f in f_words}\n",
    "        total_a = {}\n",
    "        count_a = {}\n",
    "        for l_e in e_lengths:\n",
    "            for l_f in f_lengths:\n",
    "                for j in range(0,l_e):\n",
    "                    total_a[(j,l_e,l_f)] = 0\n",
    "                    for i in range(0,l_f):\n",
    "                        count_a[(i,j,l_e,l_f)] = 0\n",
    "        for k in range (0, len(e_set)):\n",
    "            e = e_set[k]\n",
    "            f = f_set[k]\n",
    "            l_e = len(e)\n",
    "            l_f = len(f)\n",
    "            #compute normalization\n",
    "            s_total = {key: 0 for key in e_words}\n",
    "            for j in range(0, l_e):\n",
    "                for i in range(0, l_f):\n",
    "                    e_j = e[j]\n",
    "                    f_i = f[i]\n",
    "                    s_total[e_j] += t[e_j][f_i] * a[(i,j,l_e,l_f)]\n",
    "            #collect counts\n",
    "            for j in range(0, l_e):\n",
    "                for i in range(0, l_f):\n",
    "                    e_j = e[j]\n",
    "                    f_i = f[i]\n",
    "                    c = t[e_j][f_i] * a[(i,j,l_e,l_f)] / s_total[e_j]\n",
    "                    count[e_j][f_i] += c\n",
    "                    total[f_i] += c\n",
    "                    count_a[(i,j,l_e,l_f)] += c\n",
    "                    total_a[(j,l_e,l_f)] += c\n",
    "        #estimate probabilities\n",
    "        last_t = t\n",
    "        for f_i in f_words:\n",
    "            for e_j in e_words:\n",
    "                t[e_j][f_i] = count[e_j][f_i] / total[f_i]\n",
    "        for l_e in e_lengths:\n",
    "            for l_f in f_lengths:\n",
    "                for i in range(0,l_f):\n",
    "                    for j in range(0,l_e):\n",
    "                        a[(i,j,l_e,l_f)] = count_a[(i,j,l_e,l_f)] / total_a[(j,l_e,l_f)]    \n",
    "        step += 1\n",
    "        #print(t)\n",
    "    return t, a\n",
    "\n",
    "t, a = EM_IBM_Model_2(e_train,f_train)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p( ['the', 'book'] | ['das', 'Buch'] ) = 1.7040281502999115\n",
      "p( ['kgefw', 'fek'] | ['das', 'Buch'] ) = 1.7040281502999115\n"
     ]
    }
   ],
   "source": [
    "#Testing\n",
    "def prob_e_given_f (e, f, epsilon, t):\n",
    "    t_sum = np.sum([list(i.values()) for i in t.values()])\n",
    "    return ( epsilon / len(f)**len(e) ) * t_sum\n",
    "    #without +1 for NONE\n",
    "    #here problem: sum over t and t also contains unused values!\n",
    "\n",
    "for i in range(0,len(f_test)):\n",
    "    f = f_test[i]\n",
    "    e = e_test[i]\n",
    "    #decoding\n",
    "    #calculate p(e|f)\n",
    "    #print(e, prob_e_given_f(e, f, 0.001, t))\n",
    "    \n",
    "f = ['das','Buch']\n",
    "e = ['the','book']\n",
    "print('p(',e,'|',f,') =',prob_e_given_f(e, f, 1, t))\n",
    "e = ['kgefw','fek']\n",
    "print('p(',e,'|',f,') =',prob_e_given_f(e, f, 1, t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Phrase-based model\n",
    "\n",
    "#Sentence Alignments in both directions with Viterbi algorithm using IBM Model 2\n",
    "#While it is true that during the EM training fractional counts are\n",
    "#collected over a probability distribution of possible alignments, one\n",
    "#word alignment sticks out: the most probable alignment, also called the\n",
    "#Viterbi alignment\n",
    "#>  output the Viterbi alignment of the last iteration\n",
    "#> run IBM model training in both directions\n",
    "\n",
    "#for each sentence combine the two alignments using the word alignment algorithm\n",
    "#>merged by taking the intersection or the union of alignment points of each alignment\n",
    "#The algorithm starts with the intersection of the alignments. In\n",
    "#the growing step, neighboring alignment points that are in the union but\n",
    "#not in the intersection are added. In the final step, alignment points for\n",
    "#words that are still unaligned are added.\n",
    "\n",
    "#call that for every sentence pair!\n",
    "def GROW_DIAG():\n",
    "    while (last_a != a):\n",
    "        for english word e = 0 ... en\n",
    "            for foreign word f = 0 ... fn\n",
    "                if ( e aligned with f )\n",
    "                    for each neighboring point ( e-new, f-new ):\n",
    "                        if ( ( e-new not aligned or f-new not aligned ) and ( e-new, f-new ) in union( e2f, f2e ) )\n",
    "                            add alignment point ( e-new, f-new )\n",
    "\n",
    "def FINAL(a):\n",
    "    for english word e-new = 0 ... en\n",
    "        for foreign word f-new = 0 ... fn\n",
    "            if ( ( e-new not aligned or f-new not aligned ) and ( e-new, f-new ) in union( e2f, f2e) \n",
    "                add alignment point ( e-new, f-new )\n",
    "                \n",
    "def GROW_DIAG_FINAL(e2f,f2e):\n",
    "    neighboring = [(-1,0),(0,-1),(1,0),(0,1),(-1,-1),(-1,1),(1,-1),(1,1)]\n",
    "    alignment = intersect(e2f,f2e)\n",
    "    GROW_DIAG()\n",
    "    FINAL(e2f)\n",
    "    FINAL(f2e)\n",
    "#phrase extraction algorithm Use a suitable threshold for maximum phrase length and \n",
    "#estimate the phrase translation probabilities. \n",
    "\n",
    "#test the model using the test data. \n",
    "#for a source sentence, you can generate some example target sentences (word sequences) \n",
    "#and phrases “manually”; i.e. you will do the decoding process manually. \n",
    "#That is, for a source sentence, generate a possible target sentence,\n",
    "#divide both sentences into phrases as you wish, and assume that you know the phrase\n",
    "#correspondences. Then, calculate the probability of the target sentence given the source\n",
    "#sentence using the p(f|e)*p LM (e) equation (i.e. using the standard model with three\n",
    "#components). \n",
    "\n",
    "#You can train a simple bigram language model and a reordering model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
