{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run '/home/lena/Desktop/Application Project/src/ibm1.py'\n",
    "%run '/home/lena/Desktop/Application Project/src/ibm2.py'\n",
    "%run '/home/lena/Desktop/Application Project/src/pbmt.py'\n",
    "%run '/home/lena/Desktop/Application Project/src/utils.py'\n",
    "%run '/home/lena/Desktop/Application Project/src/test.py'\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrong t house Haus 0.666666296495824 0.9999991271302255\n",
      "------\n",
      "p( ['the'] | ['das'] ) = 1.0\n",
      "p( ['the', 'the', 'house', 'a', 'book'] | ['das', 'ein', 'Buch', 'Haus'] ) = 0.0011531999841871323\n",
      "p( ['book', 'a'] | ['Buch', 'ein'] ) = 0.24999999997398636\n",
      "p( ['book', 'the'] | ['Buch', 'ein'] ) = 2.601366715804536e-11\n",
      "p( ['kgefw', 'fek'] | ['Buch', 'ein'] ) = 0.0\n",
      "IBM Model 1 : 40 % correct\n",
      "IBM Model 2 : 0 % correct\n"
     ]
    }
   ],
   "source": [
    "TEST = True \n",
    "nr_used_sentences = 100 #1800 for 1model,25steps  # of 688670 lines\n",
    "path = \"./src/Paralel Corpus/\"\n",
    "e, f = read_in_corpus(nr_used_sentences, path)\n",
    "\n",
    "training_split = 0.9\n",
    "e_train, e_test = split_dataset(e, training_split)\n",
    "f_train, f_test = split_dataset(f, training_split)\n",
    "if TEST:\n",
    "    e_train, f_train, e_test, f_test, all_alignments_test = initialize_test_sets()\n",
    "#TODO or save alignments as { 0: {0:[0]}, 1: {0:[0,1], 1:[3], 2:[1], 3:[2]} } ?\n",
    "\n",
    "max_steps = 25\n",
    "\n",
    "t_e2f = EM_IBM_Model_1(e_train, f_train, max_steps)\n",
    "#TODO save t, a in files & load them?\n",
    "\n",
    "compare_ibm_1(t_e2f, max_steps, e_train, f_train)  # to test\n",
    "\n",
    "# TODO my a is 1 when it should be low :(\n",
    "t, a = EM_IBM_Model_2(e_train, f_train, t_e2f, max_steps)\n",
    "#compare_ibm_2(t, max_steps, a, e_train, f_train)  # to test\n",
    "print('------')\n",
    "t_f2e = EM_IBM_Model_1(f_train, e_train, max_steps)\n",
    "#t, a = EM_IBM_Model_2(f_train, e_train, t_f2e, max_steps)\n",
    "#compare_ibm_2(t, max_steps, a, f_train, e_train)\n",
    "\n",
    "sum = 0\n",
    "for i in range(len(f_test)):\n",
    "    if (i,4,2,2) in a:\n",
    "        sum += a[(i,4,2,2)]\n",
    "   # else:\n",
    "    #    print('tüdelü')\n",
    "#print(sum,f_test[i],e_test[4],' TODO sum is supposed to always be 1')\n",
    "\n",
    "# Testing\n",
    "#TODO p(e|f) calc is wrong!!!!\n",
    "wrong1 = 0\n",
    "wrong2 = 0\n",
    "for i in range(0, len(f_test)):\n",
    "    f = f_test[i]\n",
    "    e = e_test[i]\n",
    "    p1 = prob_e_given_f_1(e, f, 1, t_e2f)\n",
    "    if (p1 < 0.01):\n",
    "        wrong1 += 1\n",
    "    print('p(', e, '|', f, ') =', p1)\n",
    "    p2 = 0#prob_e_given_f_2(e, f, 1, t, a)\n",
    "    if (p2 < 0.5):\n",
    "        wrong2 += 1\n",
    "    #print('a: p(', e, '|', f, ') =', p2)\n",
    "    \n",
    "print('IBM Model 1 :', int(100*(1-wrong1/len(f_test))),'% correct')\n",
    "print('IBM Model 2 :', int(100*(1-wrong2/len(f_test))),'% correct')\n",
    "        \n",
    "# arg max_e p(e)p(f|e) decoding manually #TODO how???\n",
    "# calculate p(e|f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the'] ['das']\n",
      "e2f {0: 0}\n",
      "f2e {0: 0}\n",
      "inter {0: 0}\n",
      "unio {0: [0]}\n",
      "-----\n",
      "['the', 'the', 'house', 'a', 'book'] ['das', 'ein', 'Buch', 'Haus']\n",
      "e2f {0: 0, 1: 0, 2: 3, 3: 1, 4: 2}\n",
      "f2e {0: 0, 1: 3, 2: 4, 3: 2}\n",
      "inter {0: 0, 1: 3, 2: 4, 3: 2}\n",
      "unio {0: [0, 1], 1: [3], 2: [4], 3: [2]}\n",
      "-----\n",
      "['book', 'a'] ['Buch', 'ein']\n",
      "e2f {0: 0, 1: 1}\n",
      "f2e {0: 0, 1: 1}\n",
      "inter {0: 0, 1: 1}\n",
      "unio {0: [0], 1: [1]}\n",
      "-----\n",
      "['book', 'the'] ['Buch', 'ein']\n",
      "e2f {0: 0, 1: 0}\n",
      "f2e {0: 0, 1: 0}\n",
      "inter {0: 0}\n",
      "unio {0: [0, 1], 1: [0]}\n",
      "-----\n",
      "['kgefw', 'fek'] ['Buch', 'ein']\n",
      "e2f {0: None, 1: None}\n",
      "f2e {0: None, 1: None}\n",
      "inter {}\n",
      "unio {}\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "# Phrase - based MT\n",
    "def viterbi_alignment(src, tar, t):#, a):\n",
    "    #TODO what if no alignment or (src_j, tar_i) not in t?\n",
    "    #return dictionary viterbi_alignment src -> tar\n",
    "    viterbi_alignment = {}\n",
    "    l_e = len(src)\n",
    "    l_f = len(tar)\n",
    "    for (j, src_j) in enumerate(src):\n",
    "        max_value = -1\n",
    "        max_index = None\n",
    "        for (i, tar_i) in enumerate(tar):\n",
    "            if (src_j, tar_i) in t:\n",
    "                value = t[(src_j, tar_i)] #* a[(i, j, l_e, l_f)]\n",
    "                # select the first one among the maximum candidates\n",
    "                if max_value < value:\n",
    "                    max_value = value\n",
    "                    max_index = i\n",
    "        viterbi_alignment[j] = max_index\n",
    "    return viterbi_alignment\n",
    "\n",
    "e_test = [['the'],['the','the','house','a','book'],['book','a'],['book','the'],['kgefw', 'fek']]\n",
    "f_test = [['das'],['das','ein','Buch','Haus'],['Buch','ein'],['Buch','ein'],['Buch','ein']]\n",
    "all_alignments_test = {0:[[1]], 1:[[1,0,0,0],[1,0,0,0],[0,0,0,1],[0,1,0,0],[0,0,1,0]], 2:[[1,0],[0,1]], 3:[[1,0],[0,1]], 4:[[1,0],[0,1]]}\n",
    "    \n",
    "def inter(e2f, f2e):\n",
    "    intersection = {}\n",
    "    for j in range(len(e2f)):\n",
    "        i = e2f[j]\n",
    "        if i != None and j == f2e[i]:\n",
    "            intersection[j] = i\n",
    "    return intersection\n",
    "\n",
    "def unio(e2f, f2e):\n",
    "    union = {}\n",
    "    for j in range(len(e2f)):\n",
    "        i = e2f[j]\n",
    "        if i != None:\n",
    "            union[j] = union.get(j,[])\n",
    "            union[j].append(i)\n",
    "    for i in range(len(f2e)):\n",
    "        j = f2e[i]\n",
    "        if j != None:\n",
    "            union[j] = union.get(j,[])\n",
    "            if i not in union[j]: union[j].append(i)\n",
    "    return union\n",
    "\n",
    "def word_alignment(all_e2f, allf2e):\n",
    "    all_e2f ={}\n",
    "    all_f2e ={}\n",
    "    for si in range(0,len(f_test)):\n",
    "        e2f = viterbi_alignment(e_test[si], f_test[si], t_e2f)\n",
    "        f2e = viterbi_alignment(f_test[si], e_test[si], t_f2e)\n",
    "        print(e_test[si],f_test[si])\n",
    "        print('e2f',e2f)\n",
    "        print('f2e',f2e)\n",
    "        print('inter',inter(f2e, e2f)) #(e2f, f2e)\n",
    "        print('unio', unio(f2e, e2f))\n",
    "        print('-----')\n",
    "        all_e2f[si] = e2f\n",
    "        all_f2e[si] = f2e\n",
    "word_alignment(all_e2f, all_f2e)\n",
    "#In my first approach I had implemented the word alignment using the alignment matrices as in the lecture. That turned out as a reason for unnecessary big memory usage and so I decided instead to use a dict as a datastructure with a mapping from index to list of indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the'] ['das'] \n",
      "viterbi e2f: defaultdict(<class 'float'>, {1: 1}) \n",
      "viterbi: defaultdict(<class 'float'>, {1: 1})\n",
      "my f2e: [[1.]] \n",
      " my e2f: [[1.]] \n",
      "right: [[1]] \n",
      "-----\n",
      "['the', 'the', 'house', 'a', 'book'] ['das', 'ein', 'Buch', 'Haus'] \n",
      "viterbi e2f: defaultdict(<class 'float'>, {1: 1, 2: 1, 3: 4, 4: 2, 5: 3}) \n",
      "viterbi: defaultdict(<class 'float'>, {1: 1, 2: 4, 3: 5, 4: 3})\n",
      "my f2e: [[1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0.]] \n",
      " my e2f: [[1. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]] \n",
      "right: [[1, 0, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 1, 0, 0], [0, 0, 1, 0]] \n",
      "-----\n",
      "['book', 'a'] ['Buch', 'ein'] \n",
      "viterbi e2f: defaultdict(<class 'float'>, {1: 1, 2: 2}) \n",
      "viterbi: defaultdict(<class 'float'>, {1: 1, 2: 2})\n",
      "my f2e: [[1. 0.]\n",
      " [0. 1.]] \n",
      " my e2f: [[1. 0.]\n",
      " [0. 1.]] \n",
      "right: [[1, 0], [0, 1]] \n",
      "-----\n",
      "['book', 'the'] ['Buch', 'ein'] \n",
      "viterbi e2f: defaultdict(<class 'float'>, {1: 1, 2: 1}) \n",
      "viterbi: defaultdict(<class 'float'>, {1: 1, 2: 1})\n",
      "my f2e: [[1. 1.]\n",
      " [0. 0.]] \n",
      " my e2f: [[1. 1.]\n",
      " [0. 0.]] \n",
      "right: [[1, 0], [0, 1]] \n",
      "-----\n",
      "['kgefw', 'fek'] ['Buch', 'ein'] \n",
      "viterbi e2f: defaultdict(<class 'float'>, {1: 0, 2: 0}) \n",
      "viterbi: defaultdict(<class 'float'>, {1: 0, 2: 0})\n",
      "my f2e: [[1. 1.]\n",
      " [0. 0.]] \n",
      " my e2f: [[1. 1.]\n",
      " [0. 0.]] \n",
      "right: [[1, 0], [0, 1]] \n",
      "-----\n",
      "[[ True]]\n",
      "compare aligns [[1.]] [[1]]\n",
      "[[ True  True  True  True]\n",
      " [False  True  True  True]\n",
      " [ True  True  True  True]\n",
      " [ True  True  True  True]\n",
      " [ True  True  True  True]]\n",
      "compare aligns [[1. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]] [[1, 0, 0, 0], [1, 0, 0, 0], [0, 0, 0, 1], [0, 1, 0, 0], [0, 0, 1, 0]]\n",
      "[[ True  True]\n",
      " [ True  True]]\n",
      "compare aligns [[1. 0.]\n",
      " [0. 1.]] [[1, 0], [0, 1]]\n",
      "[[ True False]\n",
      " [False False]]\n",
      "compare aligns [[1. 1.]\n",
      " [1. 0.]] [[1, 0], [0, 1]]\n",
      "[[ True False]\n",
      " [False False]]\n",
      "compare aligns [[1. 1.]\n",
      " [1. 0.]] [[1, 0], [0, 1]]\n",
      "{\"(['house'], ['das'])\": 1, \"(['house'], ['das', 'Haus'])\": 9, \"(['house', 'the'], ['das'])\": 1, \"(['house', 'the'], ['das', 'Haus'])\": 9, \"(['house', 'the', 'the'], ['das'])\": 1, \"(['house', 'the', 'the'], ['das', 'Haus'])\": 9}\n"
     ]
    }
   ],
   "source": [
    "# Phrase - based MT\n",
    "\n",
    "#e2f = ibmmodel2.viterbi_alignment(fs, es, *e2f_train).items()\n",
    "#TODO interesting here: f, e but e2f\n",
    "\n",
    "import collections\n",
    "# Viterbi alignment\n",
    "# find the most probable alignment for a given translation pair\n",
    "def viterbi_alignment(es, fs, t):#, a):\n",
    "    #return dictionary e in es -> f in fs\n",
    "    max_a = collections.defaultdict(float)\n",
    "    l_e = len(es)\n",
    "    l_f = len(fs)\n",
    "    for (j, e) in enumerate(es, 1):\n",
    "        current_max = (0, -1)\n",
    "        for (i, f) in enumerate(fs, 1):\n",
    "            if (e,f) in t:\n",
    "                val = t[(e, f)] #* a[(i, j, l_e, l_f)]\n",
    "                # select the first one among the maximum candidates\n",
    "                if current_max[1] < val:\n",
    "                    current_max = (i, val)\n",
    "        max_a[j] = current_max[0]\n",
    "    return max_a\n",
    "\n",
    "# IBM Models for word alignment\n",
    "# TODO I have to use ibm model 2!!!!\n",
    "# TODO actually use e_train but here e_test only to test\n",
    "#all_aligned_e2f = align(e_train, f_train, t_e2f)\n",
    "#all_aligned_f2e = align(f_train, e_train, t_f2e)\n",
    "\n",
    "all_aligned_e2f = align(e_test, f_test, t_e2f)\n",
    "all_aligned_f2e = align(f_test, e_test, t_f2e)\n",
    "for si in range(0,len(f_test)):\n",
    "    new_all_aligned_e2f = viterbi_alignment(e_test[si], f_test[si], t_e2f)\n",
    "    new_all_aligned_f2e = viterbi_alignment(f_test[si], e_test[si], t_f2e)\n",
    "    print(e_test[si],f_test[si],'\\nviterbi e2f:',new_all_aligned_e2f,'\\nviterbi:',new_all_aligned_f2e)\n",
    "    print('my f2e:', all_aligned_f2e[si],'\\n my e2f:', all_aligned_e2f[si], '\\nright:',all_alignments_test[si],'\\n-----')\n",
    "#align = alignment(es, fs, e2f, f2e)  # symmetrized alignment\n",
    "\n",
    "# transpose so that alignments are same shape\n",
    "all_aligned_f2e = [np.transpose(f2e) for f2e in list(all_aligned_f2e.values())]\n",
    "#for sentence in range(0, len(e_train)):\n",
    "\t#print(e_train[sentence], f_train[sentence])\n",
    "\t#print(all_aligned_e2f[sentence])\n",
    "\t#print(all_aligned_f2e[sentence])\n",
    "# x foreign, y English\n",
    "\n",
    "# call for every sentence pair\n",
    "all_alignments = {}\n",
    "for index in range(0, len(e_test)):\n",
    "\te2f = all_aligned_e2f[index]\n",
    "\tf2e = all_aligned_f2e[index]\n",
    "\talignment = grow_diag_final(e2f, f2e)\n",
    "\t#print(alignment)\n",
    "\tall_alignments[index] = alignment\n",
    "    \n",
    "for index in range(len(all_alignments)):\n",
    "    print(all_alignments[index].astype(int) == all_alignments_test[index])\n",
    "    print('compare aligns',all_alignments[index], all_alignments_test[index])\n",
    "\n",
    "######\n",
    "\"\"\"\n",
    "e_train = [['michael','assumes','that','he','will','stay','in','the','house']]\n",
    "f_train = [['michael','geht','davon','aus',',','dass','er','im','haus','bleibt']]\n",
    "all_alignments = {0:[[1,0,0,0,0,0,0,0,0,0],\n",
    "                   [0,1,1,1,0,0,0,0,0,0],\n",
    "                   [0,0,0,0,0,1,0,0,0,0],\n",
    "                   [0,0,0,0,0,0,1,0,0,0],\n",
    "                   [0,0,0,0,0,0,0,0,0,1],\n",
    "                   [0,0,0,0,0,0,0,0,0,1],\n",
    "                   [0,0,0,0,0,0,0,1,0,0],\n",
    "                   [0,0,0,0,0,0,0,1,0,0],\n",
    "                   [0,0,0,0,0,0,0,0,1,0]]}\n",
    "# all_alignments[a][b] alignment for b word in e for sentence a\n",
    "\"\"\"\n",
    "\n",
    "#all_BP = phrase_extraction(e_train, f_train, all_alignments)\n",
    "# for BP in all_BP:\n",
    "#print(all_BP)\n",
    "\n",
    "# probabilistic phrase translation table\n",
    "# For each sentence pair, we extract a number of phrase pairs.\n",
    "# Then, we count in how many sentence pairs a particular phrase pair is\n",
    "# extracted and store this number in count(e,f).\n",
    "all_BP_flat = [item for sublist in all_BP for item in sublist]\n",
    "count = {}\n",
    "for (a, b) in all_BP_flat:\n",
    "\t# key = (frozenset(a),frozenset(b))\n",
    "\tkey = str((a, b))\n",
    "\tcount[key] = count.get(key, 0) + 1\n",
    "print(count)\n",
    "\n",
    "#print(φ(['house'], ['Haus'], count))\n",
    "#print(φ(['the','house'],['das','Haus'],count))\n",
    "\n",
    "# TODO bigram language model\n",
    "#smoothing in case of 0\n",
    "#just table of p(a|b) for all word combinations? train on f_trai, e_train\n",
    "# eg -0.2359 When will\n",
    "\n",
    "# Sentence Alignments in both directions with Viterbi algorithm using IBM Model 2\n",
    "# While it is true that during the EM training fractional counts are\n",
    "# collected over a probability distribution of possible alignments, one\n",
    "# word alignment sticks out: the most probable alignment, also called the\n",
    "# Viterbi alignment\n",
    "# > output the Viterbi alignment of the last iteration\n",
    "# > run IBM model training in both directions\n",
    "\n",
    "# TODO phrase-based statistical machine translation model\n",
    "# • the phrase translation table φ(f |e);\n",
    "# • the reordering model d;\n",
    "# • the language model pLM(e).\n",
    "# e_best = argmax_e prod_i φ(f|e) * d(start_i - end_i_min_1 - 1) * prod_i PLM(e_i|e_i_min_1)\n",
    "for sentence in range(0, len(all_alignments_test)):\n",
    "    f = f_test[sentence]\n",
    "    e = e_test[sentence]\n",
    "    alignment = all_alignments_test[sentence]\n",
    "    for j in range(0, len(e)):\n",
    "        e_j = e[j]\n",
    "        start_j, end_j_min_1 = calc_vals_for_d(alignment, j, f)\n",
    "        d(start_j - end_j_min_1 - 1)\n",
    "        #to test reordering\n",
    "        #print(j, start_j, end_j_min_1, start_j - end_j_min_1 - 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODOS\n",
    "\n",
    "#do summary 09!\n",
    "\n",
    "#TODO unaligned cases!\n",
    "\n",
    "#debug ibm 2\n",
    "#IBM Model 2: How to calculate a correctly?\n",
    "#implementation exactly like in book\n",
    "#eg sum is supposed to always be 1\n",
    "#use ibm model 2 in slides\n",
    "#compare subresults if they are right, count, stotal etc\n",
    "#look into indexing, starting 0 / 1?\n",
    "\n",
    "# print(φ(['house'], ['Haus'], count))\n",
    "# print(φ(['the','house'],['das','Haus']))\n",
    "# doesnt work\n",
    "\n",
    "# TODO translation probability is not working for phrases containing several words\n",
    "#p(e|f) for IBM Model 2: Gives result 4 should be 1?\n",
    "#is sum wrong maybe?\n",
    "#> compare with other implementations\n",
    "\n",
    "#decoding\n",
    "#generate some example target sentences (word sequences) “manually”\n",
    "#and calculating p(e|f) for only that, & use t for that?\n",
    "#make up f, dont implement max, just some examples\n",
    "\n",
    "#calculate Viterbi alignment for ibm2\n",
    "\n",
    "#look into efficiency issues, use hashing?\n",
    "# TODO For the estimation of the phrase translation probabilities, not all\n",
    "# phrase pairs have to be loaded into memory. It is possible to efficiently\n",
    "# estimate the probability distribution by storing and sorting the extracted\n",
    "# phrases on disk. Similarly, when using the translation table for the translation of a single sentence, only a small fraction of it is needed and may\n",
    "# be loaded on demand.\n",
    "# think about saving stuff in file\n",
    "# • Phrase translation table typically bigger than corpus\n",
    "# ... even with limits on phrase lengths (e.g., max 7 words)\n",
    "# → Too big to store in memory?\n",
    "# • Solution for training\n",
    "# – extract to disk, sort, construct for one source phrase at a time\n",
    "# • Solutions for decoding\n",
    "# – on-disk data structures with index for quick look-ups\n",
    "# – suffix arrays to create phrase pairs on demand\n",
    "# TODO use numpy for everything\n",
    "# maybe other data structure ['the','house']:[['das','Haus'],['ein','Haus']]\n",
    "# faster sometimes but slower if translation in other direction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
