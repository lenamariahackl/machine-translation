{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run '/home/lena/Desktop/Application Project/src/ibm1.py'\n",
    "%run '/home/lena/Desktop/Application Project/src/ibm2.py'\n",
    "%run '/home/lena/Desktop/Application Project/src/pbmt.py'\n",
    "%run '/home/lena/Desktop/Application Project/src/utils.py'\n",
    "%run '/home/lena/Desktop/Application Project/src/test.py'\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "TEST = False \n",
    "nr_used_sentences = 50 #1800 for 1model,25steps  # of 688670 lines\n",
    "path = \"./src/Paralel Corpus/\"\n",
    "e, f = read_in_corpus(nr_used_sentences, path)\n",
    "\n",
    "training_split = 0.9\n",
    "e_train, e_test = split_dataset(e, training_split)\n",
    "f_train, f_test = split_dataset(f, training_split)\n",
    "\n",
    "if TEST:\n",
    "    # we don't use real data but instead [['house','the','the'],['the','the','book'],['a','book']] etc\n",
    "    e_train, f_train, all_alignments_train, e_test, f_test, all_alignments_test = initialize_test_sets()\n",
    "\n",
    "max_steps = 75#25\n",
    "\n",
    "t_e2f_ibm1 = EM_IBM_Model_1(e_train, f_train, max_steps)\n",
    "t_e2f, a_e2f = EM_IBM_Model_2(e_train, f_train, t_e2f_ibm1, max_steps)\n",
    "t_f2e_ibm1 = EM_IBM_Model_1(f_train, e_train, max_steps)\n",
    "t_f2e, a_f2e = EM_IBM_Model_2(f_train, e_train, t_f2e_ibm1, max_steps)\n",
    "\n",
    "if TEST:\n",
    "    # alignment probabilities a gives different results than implementation in the nltk library\n",
    "    #compare_ibm_1_nltk(t_e2f_ibm1, max_steps, e_train, f_train)\n",
    "    #compare_ibm_2_nltk(t_e2f, max_steps, a_e2f, e_train, f_train)\n",
    "    #compare_ibm_2_nltk(t_f2e, max_steps, a_f2e, f_train, e_train)\n",
    "    \n",
    "    # compare a's train() and EM_IBM_Model_2() > 99% same a\n",
    "    #compare_a_ibm_2_train(t_e2f, max_steps, a_e2f, e_train, f_train)\n",
    "    #compare_a_ibm_2_train(t_f2e, max_steps, a_f2e, f_train, e_train)\n",
    "\n",
    "    # calculate sum of a over all words f: should be 1 \n",
    "    #test_sum_a_is_one(a_e2f, f_train)\n",
    "    #test_sum_a_is_one(a_f2e, e_train)\n",
    "    #compare_a_nltk_train(t_e2f_ibm1, max_steps, e_train, f_train)\n",
    "\n",
    "    \n",
    "\"\"\"# Testing\n",
    "# only works for test test_set, not for real data\n",
    "correct1 = 0\n",
    "correct2 = 0\n",
    "bool_test = [True, True, True, False, False, False] #correct prob values around[1, 0.7, 0.9, 0.1, 0.01, 0]\n",
    "for i in range(0, len(f_test)):\n",
    "    f = f_test[i]\n",
    "    e = e_test[i]\n",
    "    p1 = prob_e_given_f_1(e, f, 1, t_e2f_ibm1)\n",
    "    if (p1 > 0.5) == bool_test[i]: correct1 += 1\n",
    "    print('p_IBM1(', e, '|', f, ') =', p1)\n",
    "    p2 = prob_e_given_f_2(e, f, 1, t_e2f, a_e2f)\n",
    "    if (p2 > 0.5) == bool_test[i]: correct2 += 1\n",
    "    print('p_IBM2(', e, '|', f, ') =', p2)\n",
    "print('IBM Model 1 :', int(100*(correct1/len(f_test))),'% correct')\n",
    "print('IBM Model 2 :', int(100*(correct2/len(f_test))),'% correct')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phrase - based MT\n",
    "#Input: sets of sentences e and f, t- and a-table in both directions\n",
    "#Output: alignments for sentences in sets\n",
    "def word_alignment(e, f, t_e2f, a_e2f, t_f2e, a_f2e):\n",
    "    all_a = {}\n",
    "    for si in range(0, len(f)):\n",
    "        e2f = viterbi_alignment(e[si], f[si], t_e2f, a_e2f)\n",
    "        f2e = viterbi_alignment(f[si], e[si], t_f2e, a_f2e)\n",
    "        a = combine(f2e, e2f)\n",
    "        if (a == all_alignments_train[si]): print('Right alignment :)\\n')\n",
    "        else: \n",
    "            print(e[si],f[si],'\\ne2f    ',e2f,'\\nf2e    ',f2e,'\\ncombine', a)\n",
    "            print('Returned wrong alignment',a,'\\nRight alignment would be',all_alignments_train[si],'\\n')\n",
    "        all_a[si] = a\n",
    "    return all_a\n",
    "\n",
    "# IBM Model 2 for word alignment\n",
    "all_a_train = word_alignment(e_train, f_train, t_e2f, a_e2f, t_f2e, a_f2e)\n",
    "#all_a_test = word_alignment(e_test, f_test, t_e2f, a_e2f, t_f2e, a_f2e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HEREEEEE\n",
    "\"\"\"\n",
    "{['house']:{['Haus']:1}, (['house', 'the'], ['das', 'Haus']), (['house', 'the', 'the'], ['das', 'Haus']), (['the'], ['das']), (['the', 'the'], ['das']), (['the'], ['das', 'Buch']), (['the', 'the'], ['das', 'Buch']), (['the', 'the', 'book'], ['das', 'Buch']), (['a'], ['ein']), (['a', 'book'], ['ein', 'Buch']), (['book'], ['Buch']), (['house'], ['Haus'])]\n",
    "\"\"\"\n",
    "e_train = [['michael','assumes','that','he','will','stay','in','the','house']]\n",
    "f_train = [['michael','geht','davon','aus',',','dass','er','im','haus','bleibt']]\n",
    "all_a_train = {0:{0:[0],1:[1,2,3],2:[5],3:[6],4:[9],5:[9],6:[7],7:[7],8:[8]}}\n",
    "# all_a[a][b] alignment for b word in e for sentence a\n",
    "#\"\"\"\n",
    "counts = phrase_extraction(e_train, f_train, all_a_train, 3)\n",
    "#counts = phrase_extraction(e_test, f_test, all_a_test, 3)\n",
    "print('counts:',counts,'\\n')\n",
    "\n",
    "#phrase translation table\n",
    "print('PT_prob([\\'house\\'], [\\'Haus\\']) =', PT_prob(['house'], ['Haus'], counts))\n",
    "print('PT_prob([\\'the\\',\\'house\\'], [\\'das\\',\\'Haus\\']) =', PT_prob(['the','house'],['das','Haus'],counts),'\\n')\n",
    "\n",
    "#bigram language model\n",
    "unigram_counts, bigram_counts = count_grams(e_test)\n",
    "#print('unigram_counts',unigram_counts,'\\n bigram_counts', bigram_counts,'\\n')\n",
    "\n",
    "prev = 'book'\n",
    "cur = 'a'\n",
    "print('P(',cur,'|',prev,') should be high',LM_prob(prev, cur, unigram_counts, bigram_counts))\n",
    "cur = 'house'\n",
    "print('P(',cur,'|',prev,') should be low', LM_prob(prev, cur, unigram_counts, bigram_counts))\n",
    "\n",
    "# TODO phrase-based statistical machine translation model\n",
    "# • the phrase translation table φ(f|e);\n",
    "# • the reordering model d;\n",
    "# • the language model pLM(e).\n",
    "# e_best = argmax_e prod_i φ(f|e) * d(start_i - end_i_min_1 - 1) * prod_i PLM(e_i|e_i_min_1)\n",
    "for sentence in range(0, len(all_alignments_test)):\n",
    "    f = f_test[sentence]\n",
    "    e = e_test[sentence]\n",
    "    alignment = all_alignments_test[sentence]\n",
    "    prob = prob_e_given_f(e, f, alignment, counts, unigram_counts, bigram_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO unaligned cases!\n",
    "#model null alignments, some word doesnt have a word to align to \n",
    "#> just align it with the highest probable one\n",
    "\n",
    "#debug ibm 2\n",
    "#IBM Model 2: How to calculate a correctly?\n",
    "#my a is 1 when it should be low :(\n",
    "#implementation exactly like in book\n",
    "#eg sum is supposed to always be 1\n",
    "#use ibm model 2 in slides\n",
    "#compare subresults if they are right, count, stotal etc\n",
    "#look into indexing, starting 0 / 1?\n",
    "\n",
    "# TODO translation probability is not working for phrases containing several words\n",
    "#p(e|f) for IBM Model 2: Gives result 4 should be 1?\n",
    "#is sum wrong maybe?\n",
    "#> compare with other implementations\n",
    "\n",
    "#calculating p(e|f) phrase based model\n",
    "\n",
    "#decoding > example target sentences - do we even need the test set?\n",
    "#&gen alignments or manual?\n",
    "\n",
    "#TODO save t, a in files & load them?\n",
    "\n",
    "#structure code more? classes ML model, so it's intuitive to execute\n",
    "\n",
    "#remove special chars preprocessing\n",
    "#Application: Preprocessing convert Turkish characters!\n",
    "#Remove non-ASCII characters\n",
    "#lowercase\n",
    "#multi-character whitespaces to a single whitespace character\n",
    "#Remove URLs, numbers, leading and trailing whitespaces\n",
    "\n",
    "#efficient n-gram with trie\n",
    "#only store ngrams above a certain count threshold eg 4 saving 90%\n",
    "# save a significant amount of memory if we only store terms with some minimum count.\n",
    "\n",
    "#calc how long stuff takes\n",
    "#look into efficiency issues, use hashing?\n",
    "# TODO For the estimation of the phrase translation probabilities, not all\n",
    "# phrase pairs have to be loaded into memory. It is possible to efficiently\n",
    "# estimate the probability distribution by storing and sorting the extracted\n",
    "# phrases on disk. Similarly, when using the translation table for the translation of a single sentence, only a small fraction of it is needed and may\n",
    "# be loaded on demand.\n",
    "# think about saving stuff in file\n",
    "# • Phrase translation table typically bigger than corpus\n",
    "# ... even with limits on phrase lengths (e.g., max 7 words)\n",
    "# → Too big to store in memory?\n",
    "# • Solution for training\n",
    "# – extract to disk, sort, construct for one source phrase at a time\n",
    "# • Solutions for decoding\n",
    "# – on-disk data structures with index for quick look-ups\n",
    "# – suffix arrays to create phrase pairs on demand\n",
    "# TODO use numpy for everything\n",
    "# maybe other data structure ['the','house']:[['das','Haus'],['ein','Haus']]\n",
    "# faster sometimes but slower if translation in other direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training IBM Model 1\n",
      "step 25 of 30\n",
      "IBM Model 1 training finished.\n",
      "start training IBM Model 2\n",
      "step 25 of 30\n",
      "IBM Model 2 training finished.\n",
      "start training IBM Model 1\n",
      "step 25 of 30\n",
      "IBM Model 1 training finished.\n",
      "start training IBM Model 2\n",
      "step 25 of 30\n",
      "IBM Model 2 training finished.\n",
      "IBM Model 1 :p( ['the', 'man', 'understood', 'this', 'extinguished', 'his', 'cigarette', 'went', 'to', 'the', 'hallway', 'after', 'shaking', 'the', 'womans', 'hand'] | ['adam', 'bunu', 'anladı', 'sigarasını', 'söndürdü', 'kadının', 'elini', 'sıkarak', 'hole', 'çıktı'] ) = 2.4423984532665256e-20\n",
      "IBM Model 2: p( ['the', 'man', 'understood', 'this', 'extinguished', 'his', 'cigarette', 'went', 'to', 'the', 'hallway', 'after', 'shaking', 'the', 'womans', 'hand'] | ['adam', 'bunu', 'anladı', 'sigarasını', 'söndürdü', 'kadının', 'elini', 'sıkarak', 'hole', 'çıktı'] ) = 6.721854537670273e-13\n",
      "Some contained words were not in the training set.\n",
      "IBM Model 1 :p( ['the', 'man', 'understood', 'this', 'extinguished', 'his', 'cigarette', 'went', 'to', 'the', 'hall', 'while', 'shaking', 'hands', 'with', 'the', 'woman'] | ['adam', 'bunu', 'anladı', 'sigarasını', 'söndürdü', 'kadının', 'elini', 'sıkarak', 'hole', 'çıktı'] ) = 3.1559376057768556e-40\n",
      "Some contained words were not in the training set.\n",
      "IBM Model 2: p( ['the', 'man', 'understood', 'this', 'extinguished', 'his', 'cigarette', 'went', 'to', 'the', 'hall', 'while', 'shaking', 'hands', 'with', 'the', 'woman'] | ['adam', 'bunu', 'anladı', 'sigarasını', 'söndürdü', 'kadının', 'elini', 'sıkarak', 'hole', 'çıktı'] ) = 3.8716775580310836e-245\n",
      "Some contained words were not in the training set.\n",
      "IBM Model 1 :p( ['the', 'man', 'got', 'it', 'and', 'put', 'the', 'cigarette', 'off', 'went', 'to', 'the', 'hallway', 'shaking', 'the', 'hand', 'of', 'the', 'woman'] | ['adam', 'bunu', 'anladı', 'sigarasını', 'söndürdü', 'kadının', 'elini', 'sıkarak', 'hole', 'çıktı'] ) = 5.413257271717057e-29\n",
      "Some contained words were not in the training set.\n",
      "IBM Model 2: p( ['the', 'man', 'got', 'it', 'and', 'put', 'the', 'cigarette', 'off', 'went', 'to', 'the', 'hallway', 'shaking', 'the', 'hand', 'of', 'the', 'woman'] | ['adam', 'bunu', 'anladı', 'sigarasını', 'söndürdü', 'kadının', 'elini', 'sıkarak', 'hole', 'çıktı'] ) = 1e-12\n",
      "Some contained words were not in the training set.\n",
      "IBM Model 1 :p( ['in', 'every', 'work', 'he', 'tried', 'he', 'struggled', 'and', 'was', 'working', 'in', 'the', 'railway', 'department', 'now'] | ['girdiği', 'bütün', 'işlerde', 'bir', 'terslikle', 'karşılaşmış', 'şimdi', 'de', 'demiryolu', 'işletmesine', 'geçmişti'] ) = 3.841008092749461e-134\n",
      "Some contained words were not in the training set.\n",
      "IBM Model 2: p( ['in', 'every', 'work', 'he', 'tried', 'he', 'struggled', 'and', 'was', 'working', 'in', 'the', 'railway', 'department', 'now'] | ['girdiği', 'bütün', 'işlerde', 'bir', 'terslikle', 'karşılaşmış', 'şimdi', 'de', 'demiryolu', 'işletmesine', 'geçmişti'] ) = 1e-12\n",
      "Some contained words were not in the training set.\n",
      "IBM Model 1 :p( ['in', 'every', 'work', 'he', 'entered', 'he', 'came', 'across', 'a', 'struggle', 'so', 'now', 'he', 'switched', 'into', 'the', 'railroad', 'business'] | ['girdiği', 'bütün', 'işlerde', 'bir', 'terslikle', 'karşılaşmış', 'şimdi', 'de', 'demiryolu', 'işletmesine', 'geçmişti'] ) = 7.271378628282564e-194\n",
      "Some contained words were not in the training set.\n",
      "IBM Model 2: p( ['in', 'every', 'work', 'he', 'entered', 'he', 'came', 'across', 'a', 'struggle', 'so', 'now', 'he', 'switched', 'into', 'the', 'railroad', 'business'] | ['girdiği', 'bütün', 'işlerde', 'bir', 'terslikle', 'karşılaşmış', 'şimdi', 'de', 'demiryolu', 'işletmesine', 'geçmişti'] ) = 1e-12\n",
      "Some contained words were not in the training set.\n",
      "IBM Model 1 :p( ['in', 'every', 'job', 'he', 'tried', 'he', 'was', 'misfortunate', 'and', 'now', 'he', 'switched', 'to', 'working', 'in', 'the', 'railroad', 'business'] | ['girdiği', 'bütün', 'işlerde', 'bir', 'terslikle', 'karşılaşmış', 'şimdi', 'de', 'demiryolu', 'işletmesine', 'geçmişti'] ) = 2.8065172819248475e-227\n",
      "Some contained words were not in the training set.\n",
      "IBM Model 2: p( ['in', 'every', 'job', 'he', 'tried', 'he', 'was', 'misfortunate', 'and', 'now', 'he', 'switched', 'to', 'working', 'in', 'the', 'railroad', 'business'] | ['girdiği', 'bütün', 'işlerde', 'bir', 'terslikle', 'karşılaşmış', 'şimdi', 'de', 'demiryolu', 'işletmesine', 'geçmişti'] ) = 1e-12\n",
      "Some contained words were not in the training set.\n",
      "IBM Model 1 :p( ['he', 'had', 'ruined', 'his', 'prospects', 'in', 'a', 'number', 'of', 'positions', 'and', 'was', 'now', 'serving', 'in', 'the', 'railway', 'department'] | ['girdiği', 'bütün', 'işlerde', 'bir', 'terslikle', 'karşılaşmış', 'şimdi', 'de', 'demiryolu', 'işletmesine', 'geçmişti'] ) = 1.912063473404907e-287\n",
      "Some contained words were not in the training set.\n",
      "IBM Model 2: p( ['he', 'had', 'ruined', 'his', 'prospects', 'in', 'a', 'number', 'of', 'positions', 'and', 'was', 'now', 'serving', 'in', 'the', 'railway', 'department'] | ['girdiği', 'bütün', 'işlerde', 'bir', 'terslikle', 'karşılaşmış', 'şimdi', 'de', 'demiryolu', 'işletmesine', 'geçmişti'] ) = 1e-12\n",
      "Some contained words were not in the training set.\n",
      "IBM Model 1 :p( ['Between', 'different', 'trials', 'he', 'would', 'drink', 'his', 'tea', 'smoke', 'his', 'cigarette', 'and', 'talk', 'about', 'some', 'politics', 'daily', 'errands', 'some', 'cardgames', 'and', 'mostly', 'about', 'appointments'] | ['duruşma', 'aralarında', 'çayım', 'içip', 'sigarasını', 'tüttürerek', 'biraz', 'siyasetten', 'biraz', 'günlük', 'işlerden', 'biraz', 'kâğıt', 'oyunlarından', 'daha', 'çok', 'da', 'atamalardan', 'söz', 'açtığı', 'olurdu'] ) = 2.513074844071021e-189\n",
      "Some contained words were not in the training set.\n",
      "IBM Model 2: p( ['Between', 'different', 'trials', 'he', 'would', 'drink', 'his', 'tea', 'smoke', 'his', 'cigarette', 'and', 'talk', 'about', 'some', 'politics', 'daily', 'errands', 'some', 'cardgames', 'and', 'mostly', 'about', 'appointments'] | ['duruşma', 'aralarında', 'çayım', 'içip', 'sigarasını', 'tüttürerek', 'biraz', 'siyasetten', 'biraz', 'günlük', 'işlerden', 'biraz', 'kâğıt', 'oyunlarından', 'daha', 'çok', 'da', 'atamalardan', 'söz', 'açtığı', 'olurdu'] ) = 1e-12\n",
      "Some contained words were not in the training set.\n",
      "IBM Model 1 :p( ['in', 'the', 'intervals', 'between', 'the', 'sessions', 'he', 'smoked', 'drank', 'tea', 'chatted', 'a', 'little', 'about', 'politics', 'a', 'little', 'about', 'general', 'topics', 'a', 'little', 'about', 'cards', 'but', 'most', 'of', 'all', 'about', 'official', 'appointments'] | ['duruşma', 'aralarında', 'çayım', 'içip', 'sigarasını', 'tüttürerek', 'biraz', 'siyasetten', 'biraz', 'günlük', 'işlerden', 'biraz', 'kâğıt', 'oyunlarından', 'daha', 'çok', 'da', 'atamalardan', 'söz', 'açtığı', 'olurdu'] ) = 1.8716512302289738e-286\n",
      "Some contained words were not in the training set.\n",
      "IBM Model 2: p( ['in', 'the', 'intervals', 'between', 'the', 'sessions', 'he', 'smoked', 'drank', 'tea', 'chatted', 'a', 'little', 'about', 'politics', 'a', 'little', 'about', 'general', 'topics', 'a', 'little', 'about', 'cards', 'but', 'most', 'of', 'all', 'about', 'official', 'appointments'] | ['duruşma', 'aralarında', 'çayım', 'içip', 'sigarasını', 'tüttürerek', 'biraz', 'siyasetten', 'biraz', 'günlük', 'işlerden', 'biraz', 'kâğıt', 'oyunlarından', 'daha', 'çok', 'da', 'atamalardan', 'söz', 'açtığı', 'olurdu'] ) = 1e-12\n"
     ]
    }
   ],
   "source": [
    "####### IBM MODELS #######\n",
    "\n",
    "# read in data\n",
    "nr_used_sentences = 1800\n",
    "path = \"./src/Paralel Corpus/\"\n",
    "e, f = read_in_corpus(nr_used_sentences, path)\n",
    "\n",
    "training_split = 0.9\n",
    "e_train, e_test = split_dataset(e, training_split)\n",
    "f_train, f_test = split_dataset(f, training_split)\n",
    "max_steps = 30\n",
    "\n",
    "#TODO remove\n",
    "e_train += [['the' ,'man', 'understood','this','extinguished','his','cigarette','went','to','the','hallway','after','shaking','the','womans','hand']]\n",
    "f_train += [['adam', 'bunu', 'anladı', 'sigarasını', 'söndürdü', 'kadının', 'elini', 'sıkarak', 'hole', 'çıktı']]\n",
    "\n",
    "# execute IBM Model 1 and 2 each two times (English-Turkish and Turkish-English)\n",
    "t_e2f_ibm1 = EM_IBM_Model_1(e_train, f_train, max_steps)\n",
    "t_e2f, a_e2f = EM_IBM_Model_2(e_train, f_train, t_e2f_ibm1, max_steps)\n",
    "t_f2e_ibm1 = EM_IBM_Model_1(f_train, e_train, max_steps)\n",
    "t_f2e, a_f2e = EM_IBM_Model_2(f_train, e_train, t_f2e_ibm1, max_steps)\n",
    "\n",
    "# testing\n",
    "examples_f = [['adam', 'bunu', 'anladı', 'sigarasını', 'söndürdü', 'kadının', 'elini', 'sıkarak', 'hole', 'çıktı'],['girdiği', 'bütün', 'işlerde', 'bir', 'terslikle', 'karşılaşmış', 'şimdi', 'de', 'demiryolu', 'işletmesine', 'geçmişti'], ['duruşma', 'aralarında', 'çayım', 'içip', 'sigarasını', 'tüttürerek', 'biraz', 'siyasetten', 'biraz', 'günlük', 'işlerden', 'biraz', 'kâğıt', 'oyunlarından', 'daha', 'çok', 'da', 'atamalardan', 'söz', 'açtığı', 'olurdu']]\n",
    "examples_e = [{0: ['the' ,'man', 'understood','this','extinguished','his','cigarette','went','to','the','hallway','after','shaking','the','womans','hand'], 1: ['the' ,'man', 'understood','this','extinguished','his','cigarette','went','to','the','hall','while','shaking','hands','with','the','woman'], 2: ['the' ,'man', 'got','it','and','put','the','cigarette','off','went','to','the','hallway','shaking','the','hand','of','the','woman']},\\\n",
    "            {0:['in','every','work','he','tried','he','struggled','and','was','working','in','the','railway','department','now'],1:['in','every','work','he','entered','he','came','across','a','struggle','so','now','he','switched','into','the','railroad','business'], 2:['in','every','job','he','tried','he','was','misfortunate','and','now','he','switched','to','working','in','the','railroad','business'], 3:['he', 'had', 'ruined', 'his', 'prospects', 'in', 'a', 'number', 'of', 'positions', 'and', 'was', 'now', 'serving', 'in', 'the', 'railway', 'department']},\\\n",
    "            {0:['Between','different','trials','he','would','drink','his','tea','smoke','his','cigarette','and','talk','about','some','politics','daily','errands','some','cardgames','and','mostly','about','appointments'], 1:['in','the', 'intervals', 'between', 'the', 'sessions', 'he', 'smoked', 'drank', 'tea', 'chatted', 'a', 'little', 'about', 'politics','a', 'little', 'about', 'general', 'topics', 'a', 'little' ,'about','cards', 'but', 'most', 'of', 'all', 'about', 'official', 'appointments']}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IBM Model 1 :p( ['the', 'man', 'understood', 'this', 'extinguished', 'his', 'cigarette', 'went', 'to', 'the', 'hallway', 'after', 'shaking', 'the', 'womans', 'hand'] | ['adam', 'bunu', 'anladı', 'sigarasını', 'söndürdü', 'kadının', 'elini', 'sıkarak', 'hole', 'çıktı'] ) = 0.24423984532665258\n",
      "IBM Model 2: p( ['the', 'man', 'understood', 'this', 'extinguished', 'his', 'cigarette', 'went', 'to', 'the', 'hallway', 'after', 'shaking', 'the', 'womans', 'hand'] | ['adam', 'bunu', 'anladı', 'sigarasını', 'söndürdü', 'kadının', 'elini', 'sıkarak', 'hole', 'çıktı'] ) = 0.6721854537670273\n",
      "Some contained words were not in the training set.\n",
      "IBM Model 1 :p( ['the', 'man', 'understood', 'this', 'extinguished', 'his', 'cigarette', 'went', 'to', 'the', 'hall', 'while', 'shaking', 'hands', 'with', 'the', 'woman'] | ['adam', 'bunu', 'anladı', 'sigarasını', 'söndürdü', 'kadının', 'elini', 'sıkarak', 'hole', 'çıktı'] ) = 3.1559376057768554e-21\n",
      "Some contained words were not in the training set.\n",
      "IBM Model 2: p( ['the', 'man', 'understood', 'this', 'extinguished', 'his', 'cigarette', 'went', 'to', 'the', 'hall', 'while', 'shaking', 'hands', 'with', 'the', 'woman'] | ['adam', 'bunu', 'anladı', 'sigarasını', 'söndürdü', 'kadının', 'elini', 'sıkarak', 'hole', 'çıktı'] ) = 3.871677558031084e-233\n",
      "Some contained words were not in the training set.\n",
      "IBM Model 1 :p( ['the', 'man', 'got', 'it', 'and', 'put', 'the', 'cigarette', 'off', 'went', 'to', 'the', 'hallway', 'shaking', 'the', 'hand', 'of', 'the', 'woman'] | ['adam', 'bunu', 'anladı', 'sigarasını', 'söndürdü', 'kadının', 'elini', 'sıkarak', 'hole', 'çıktı'] ) = 5.413257271717057e-10\n",
      "Some contained words were not in the training set.\n",
      "IBM Model 2: p( ['the', 'man', 'got', 'it', 'and', 'put', 'the', 'cigarette', 'off', 'went', 'to', 'the', 'hallway', 'shaking', 'the', 'hand', 'of', 'the', 'woman'] | ['adam', 'bunu', 'anladı', 'sigarasını', 'söndürdü', 'kadının', 'elini', 'sıkarak', 'hole', 'çıktı'] ) = 0.0\n",
      "Some contained words were not in the training set.\n",
      "IBM Model 1 :p( ['in', 'every', 'work', 'he', 'tried', 'he', 'struggled', 'and', 'was', 'working', 'in', 'the', 'railway', 'department', 'now'] | ['girdiği', 'bütün', 'işlerde', 'bir', 'terslikle', 'karşılaşmış', 'şimdi', 'de', 'demiryolu', 'işletmesine', 'geçmişti'] ) = 3.841008092749461e-115\n",
      "Some contained words were not in the training set.\n",
      "IBM Model 2: p( ['in', 'every', 'work', 'he', 'tried', 'he', 'struggled', 'and', 'was', 'working', 'in', 'the', 'railway', 'department', 'now'] | ['girdiği', 'bütün', 'işlerde', 'bir', 'terslikle', 'karşılaşmış', 'şimdi', 'de', 'demiryolu', 'işletmesine', 'geçmişti'] ) = 0.0\n",
      "Some contained words were not in the training set.\n",
      "IBM Model 1 :p( ['in', 'every', 'work', 'he', 'entered', 'he', 'came', 'across', 'a', 'struggle', 'so', 'now', 'he', 'switched', 'into', 'the', 'railroad', 'business'] | ['girdiği', 'bütün', 'işlerde', 'bir', 'terslikle', 'karşılaşmış', 'şimdi', 'de', 'demiryolu', 'işletmesine', 'geçmişti'] ) = 7.271378628282564e-175\n",
      "Some contained words were not in the training set.\n",
      "IBM Model 2: p( ['in', 'every', 'work', 'he', 'entered', 'he', 'came', 'across', 'a', 'struggle', 'so', 'now', 'he', 'switched', 'into', 'the', 'railroad', 'business'] | ['girdiği', 'bütün', 'işlerde', 'bir', 'terslikle', 'karşılaşmış', 'şimdi', 'de', 'demiryolu', 'işletmesine', 'geçmişti'] ) = 0.0\n",
      "Some contained words were not in the training set.\n",
      "IBM Model 1 :p( ['in', 'every', 'job', 'he', 'tried', 'he', 'was', 'misfortunate', 'and', 'now', 'he', 'switched', 'to', 'working', 'in', 'the', 'railroad', 'business'] | ['girdiği', 'bütün', 'işlerde', 'bir', 'terslikle', 'karşılaşmış', 'şimdi', 'de', 'demiryolu', 'işletmesine', 'geçmişti'] ) = 2.8065172819248475e-208\n",
      "Some contained words were not in the training set.\n",
      "IBM Model 2: p( ['in', 'every', 'job', 'he', 'tried', 'he', 'was', 'misfortunate', 'and', 'now', 'he', 'switched', 'to', 'working', 'in', 'the', 'railroad', 'business'] | ['girdiği', 'bütün', 'işlerde', 'bir', 'terslikle', 'karşılaşmış', 'şimdi', 'de', 'demiryolu', 'işletmesine', 'geçmişti'] ) = 0.0\n",
      "Some contained words were not in the training set.\n",
      "IBM Model 1 :p( ['he', 'had', 'ruined', 'his', 'prospects', 'in', 'a', 'number', 'of', 'positions', 'and', 'was', 'now', 'serving', 'in', 'the', 'railway', 'department'] | ['girdiği', 'bütün', 'işlerde', 'bir', 'terslikle', 'karşılaşmış', 'şimdi', 'de', 'demiryolu', 'işletmesine', 'geçmişti'] ) = 1.9120634734049074e-268\n",
      "Some contained words were not in the training set.\n",
      "IBM Model 2: p( ['he', 'had', 'ruined', 'his', 'prospects', 'in', 'a', 'number', 'of', 'positions', 'and', 'was', 'now', 'serving', 'in', 'the', 'railway', 'department'] | ['girdiği', 'bütün', 'işlerde', 'bir', 'terslikle', 'karşılaşmış', 'şimdi', 'de', 'demiryolu', 'işletmesine', 'geçmişti'] ) = 0.0\n",
      "Some contained words were not in the training set.\n",
      "IBM Model 1 :p( ['Between', 'different', 'trials', 'he', 'would', 'drink', 'his', 'tea', 'smoke', 'his', 'cigarette', 'and', 'talk', 'about', 'some', 'politics', 'daily', 'errands', 'some', 'cardgames', 'and', 'mostly', 'about', 'appointments'] | ['duruşma', 'aralarında', 'çayım', 'içip', 'sigarasını', 'tüttürerek', 'biraz', 'siyasetten', 'biraz', 'günlük', 'işlerden', 'biraz', 'kâğıt', 'oyunlarından', 'daha', 'çok', 'da', 'atamalardan', 'söz', 'açtığı', 'olurdu'] ) = 2.5130748440710205e-170\n",
      "Some contained words were not in the training set.\n",
      "IBM Model 2: p( ['Between', 'different', 'trials', 'he', 'would', 'drink', 'his', 'tea', 'smoke', 'his', 'cigarette', 'and', 'talk', 'about', 'some', 'politics', 'daily', 'errands', 'some', 'cardgames', 'and', 'mostly', 'about', 'appointments'] | ['duruşma', 'aralarında', 'çayım', 'içip', 'sigarasını', 'tüttürerek', 'biraz', 'siyasetten', 'biraz', 'günlük', 'işlerden', 'biraz', 'kâğıt', 'oyunlarından', 'daha', 'çok', 'da', 'atamalardan', 'söz', 'açtığı', 'olurdu'] ) = 0.0\n",
      "Some contained words were not in the training set.\n",
      "IBM Model 1 :p( ['in', 'the', 'intervals', 'between', 'the', 'sessions', 'he', 'smoked', 'drank', 'tea', 'chatted', 'a', 'little', 'about', 'politics', 'a', 'little', 'about', 'general', 'topics', 'a', 'little', 'about', 'cards', 'but', 'most', 'of', 'all', 'about', 'official', 'appointments'] | ['duruşma', 'aralarında', 'çayım', 'içip', 'sigarasını', 'tüttürerek', 'biraz', 'siyasetten', 'biraz', 'günlük', 'işlerden', 'biraz', 'kâğıt', 'oyunlarından', 'daha', 'çok', 'da', 'atamalardan', 'söz', 'açtığı', 'olurdu'] ) = 1.8716512302289736e-267\n",
      "Some contained words were not in the training set.\n",
      "IBM Model 2: p( ['in', 'the', 'intervals', 'between', 'the', 'sessions', 'he', 'smoked', 'drank', 'tea', 'chatted', 'a', 'little', 'about', 'politics', 'a', 'little', 'about', 'general', 'topics', 'a', 'little', 'about', 'cards', 'but', 'most', 'of', 'all', 'about', 'official', 'appointments'] | ['duruşma', 'aralarında', 'çayım', 'içip', 'sigarasını', 'tüttürerek', 'biraz', 'siyasetten', 'biraz', 'günlük', 'işlerden', 'biraz', 'kâğıt', 'oyunlarından', 'daha', 'çok', 'da', 'atamalardan', 'söz', 'açtığı', 'olurdu'] ) = 0.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, len(examples_f)):\n",
    "    f = examples_f[i]\n",
    "    for e in examples_e[i].values():\n",
    "        p1 = prob_e_given_f_1(e, f, 1e19, t_e2f_ibm1)\n",
    "        print('IBM Model 1 :p(', e, '|', f, ') =', p1)\n",
    "        p2 = prob_e_given_f_2(e, f, 1e12, t_e2f, a_e2f)\n",
    "        print('IBM Model 2: p(', e, '|', f, ') =', p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_grams\n",
      "calculate PT_prob for ['the', 'man'] ['adam'] = 0.5\n",
      "d( 0 )= 1.0\n",
      "calculate PT_prob for ['understood'] ['anladı'] = 0.5\n",
      "d( 1 )= 0.5\n",
      "calculate PT_prob for ['this'] ['bunu'] = 0.00641025641025641\n",
      "d( -2 )= 0.25\n",
      "calculate PT_prob for ['extinguished', 'his', 'cigarette'] ['sigarasını', 'söndürdü'] = 1.0\n",
      "d( 1 )= 0.5\n",
      "calculate PT_prob for ['went', 'to', 'the', 'hallway'] ['hole', 'çıktı'] = 1.0\n",
      "d( 3 )= 0.125\n",
      "calculate PT_prob for ['after', 'shaking', 'the', 'womans', 'hand'] ['kadının', 'elini', 'sıkarak'] = 1.0\n",
      "d( -5 )= 0.03125\n",
      "LM_prob( the | START )= 0.026390041493775933\n",
      "LM_prob( man | the )= 0.00047393364928909954\n",
      "LM_prob( understood | man )= 0.00044974139869574995\n",
      "LM_prob( this | understood )= 0.00045351473922902497\n",
      "LM_prob( extinguished | this )= 0.00041675349031048136\n",
      "LM_prob( his | extinguished )= 0.0004540295119182747\n",
      "LM_prob( cigarette | his )= 0.0006561679790026247\n",
      "LM_prob( went | cigarette )= 0.0004538234626730202\n",
      "LM_prob( to | went )= 0.0006804264005443411\n",
      "LM_prob( the | to )= 0.019974577810059922\n",
      "LM_prob( hallway | the )= 0.000315955766192733\n",
      "LM_prob( after | hallway )= 0.0004540295119182747\n",
      "LM_prob( shaking | after )= 0.00045045045045045046\n",
      "LM_prob( the | shaking )= 0.0004540295119182747\n",
      "LM_prob( womans | the )= 0.000315955766192733\n",
      "LM_prob( hand | womans )= 0.0004538234626730202\n",
      "LM_prob( END | hand )= 0.00045320643553138455\n",
      "p( ['the', 'man', 'understood', 'this', 'extinguished', 'his', 'cigarette', 'went', 'to', 'the', 'hallway', 'after', 'shaking', 'the', 'womans', 'hand'] | ['adam', 'bunu', 'anladı', 'sigarasını', 'söndürdü', 'kadının', 'elini', 'sıkarak', 'hole', 'çıktı'] ) = 1.4572772999949208e-60 \n",
      "\n",
      "calculate PT_prob for ['the', 'man'] ['adam'] = 0.5\n",
      "d( 0 )= 1.0\n",
      "calculate PT_prob for ['understood'] ['anladı'] = 0.5\n",
      "d( 1 )= 0.5\n",
      "calculate PT_prob for ['this'] ['bunu'] = 0.00641025641025641\n",
      "d( -2 )= 0.25\n",
      "calculate PT_prob for ['extinguished', 'his', 'cigarette'] ['sigarasını', 'söndürdü'] = 1.0\n",
      "d( 1 )= 0.5\n",
      "calculate PT_prob for ['went', 'to', 'the', 'hall'] ['hole', 'çıktı'] = 1e-12\n",
      "tutu False ['went', 'to', 'the', 'hall'] ['hole', 'çıktı']\n",
      "d( 3 )= 0.125\n",
      "calculate PT_prob for ['while', 'shaking', 'hands', 'with', 'the', 'woman'] ['kadının', 'elini', 'sıkarak'] = 1e-12\n",
      "tutu False ['while', 'shaking', 'hands', 'with', 'the', 'woman'] ['kadının', 'elini', 'sıkarak']\n",
      "d( -5 )= 0.03125\n",
      "LM_prob( the | START )= 0.026390041493775933\n",
      "LM_prob( man | the )= 0.00047393364928909954\n",
      "LM_prob( understood | man )= 0.00044974139869574995\n",
      "LM_prob( this | understood )= 0.00045351473922902497\n",
      "LM_prob( extinguished | this )= 0.00041675349031048136\n",
      "LM_prob( his | extinguished )= 0.0004540295119182747\n",
      "LM_prob( cigarette | his )= 0.0006561679790026247\n",
      "LM_prob( went | cigarette )= 0.0004538234626730202\n",
      "LM_prob( to | went )= 0.0006804264005443411\n",
      "LM_prob( the | to )= 0.019974577810059922\n",
      "LM_prob( hall | the )= 0.000315955766192733\n",
      "LM_prob( while | hall )= 0.00022701475595913735\n",
      "LM_prob( shaking | while )= 0.00022624434389140272\n",
      "LM_prob( hands | shaking )= 0.00022701475595913735\n",
      "LM_prob( with | hands )= 0.00045320643553138455\n",
      "LM_prob( the | with )= 0.0060790273556231\n",
      "LM_prob( woman | the )= 0.00047393364928909954\n",
      "LM_prob( END | woman )= 0.0018013960819635217\n",
      "p( ['the', 'man', 'understood', 'this', 'extinguished', 'his', 'cigarette', 'went', 'to', 'the', 'hall', 'while', 'shaking', 'hands', 'with', 'the', 'woman'] | ['adam', 'bunu', 'anladı', 'sigarasını', 'söndürdü', 'kadının', 'elini', 'sıkarak', 'hole', 'çıktı'] ) = 6.623088208401472e-87 \n",
      "\n",
      "calculate PT_prob for ['the', 'man'] ['adam'] = 0.5\n",
      "d( 0 )= 1.0\n",
      "calculate PT_prob for ['got'] ['anladı'] = 1e-12\n",
      "tutu False ['got'] ['anladı']\n",
      "d( 1 )= 0.5\n",
      "calculate PT_prob for ['it'] ['bunu'] = 1e-12\n",
      "tutu True ['it'] ['bunu']\n",
      "{\"['ulaşamadıklarını', 'göstermektir']\": 2, \"['değerlerden']\": 1, \"['batıl', 'dinlerini']\": 1, \"['ve', 'din']\": 1, \"['vardır']\": 1, \"['zevki']\": 1, \"['değerlendirirler']\": 1, \"['hangisinin']\": 1, \"['yalnızca', 'ol']\": 1, \"['bu']\": 8, \"['kültüre']\": 1, \"['ve']\": 1, \"['hayat']\": 1, \"['olduğu']\": 2, \"['hayatlarına']\": 1, \"['huzursuz']\": 1, \"['yaşadığı']\": 1, \"['kaprisli']\": 1, \"['öğrenir']\": 1, \"['açık']\": 1, \"['sevilmesini']\": 1, \"['kimseleri']\": 1, \"['görmek']\": 1, \"['ama']\": 1, \"['izin']\": 1, \"['yapıpettikleriniz']\": 1, \"['saatler']\": 1, \"['iş']\": 1, \"['olduğunda']\": 1, \"['yine']\": 1, \"['amaçsızlıkları']\": 1, \"['erkekliğin']\": 1, \"['girmesi']\": 1, \"['yoktur']\": 1, \"['bir']\": 2, \"['ilerleyen']\": 1, \"['yaşanan']\": 1, \"['onlara']\": 1, \"['yandan']\": 1, \"['olmaz']\": 1, \"['yememeleri']\": 1, \"['karakterini']\": 1, \"['şunu']\": 2, \"['ona']\": 1, \"['olmaya']\": 1, \"['inandıkları']\": 1, \"['duymazlar']\": 1, \"['elbette']\": 1, \"['örgüsü']\": 1, \"['hatta']\": 1, \"['kalabalık']\": 1, \"['saygın']\": 1, \"['dış']\": 1, \"['maddi']\": 1, \"['işi']\": 1, \"['gelirse']\": 1, \"['sebebiyle']\": 1, \"['sevmek']\": 1, \"['içermez']\": 1, \"['zengin']\": 1, \"['çok']\": 1, \"['özellikle']\": 1, \"['tarif']\": 1, \"['tavır']\": 1, \"['işler', 'yolunda']\": 1, \"['yarar']\": 1, \"['dünyasının']\": 1, \"['mantık']\": 1, \"['eder']\": 1, \"['kompleks']\": 1, \"['doğaüstü']\": 1, \"['ancak']\": 1, \"['anlaşıldı']\": 1, \"['üç', 'boyutlu']\": 1, \"['gözün']\": 1, \"['biçimde']\": 1, \"['yaşamadıklarını']\": 1, \"['insanlar']\": 1, \"['türünün']\": 1, \"['savunmaya']\": 1, \"['insanlardır']\": 1, \"['fiyodor']\": 1, \"['kaçınılmazlığını']\": 1, \"['kadın']\": 1, \"['tülü']\": 1}\n",
      "d( -2 )= 0.25\n",
      "calculate PT_prob for ['and', 'put', 'the', 'cigarette', 'off'] ['sigarasını', 'söndürdü'] = 1e-12\n",
      "tutu False ['and', 'put', 'the', 'cigarette', 'off'] ['sigarasını', 'söndürdü']\n",
      "d( 1 )= 0.5\n",
      "calculate PT_prob for ['went', 'to', 'the', 'hallway'] ['hole', 'çıktı'] = 1.0\n",
      "d( 3 )= 0.125\n",
      "calculate PT_prob for ['shaking', 'the', 'hand', 'of', 'the', 'woman'] ['kadının', 'elini', 'sıkarak'] = 1e-12\n",
      "tutu False ['shaking', 'the', 'hand', 'of', 'the', 'woman'] ['kadının', 'elini', 'sıkarak']\n",
      "d( -5 )= 0.03125\n",
      "LM_prob( the | START )= 0.026390041493775933\n",
      "LM_prob( man | the )= 0.00047393364928909954\n",
      "LM_prob( got | man )= 0.00022487069934787497\n",
      "LM_prob( it | got )= 0.00022701475595913735\n",
      "LM_prob( and | it )= 0.0021290185224611454\n",
      "LM_prob( put | and )= 0.0003726476616359232\n",
      "LM_prob( the | put )= 0.0004522840343735866\n",
      "LM_prob( cigarette | the )= 0.0001579778830963665\n",
      "LM_prob( off | cigarette )= 0.0002269117313365101\n",
      "LM_prob( went | off )= 0.00022670596236681024\n",
      "LM_prob( to | went )= 0.0006804264005443411\n",
      "LM_prob( the | to )= 0.019974577810059922\n",
      "LM_prob( hallway | the )= 0.000315955766192733\n",
      "LM_prob( shaking | hallway )= 0.00022701475595913735\n",
      "LM_prob( the | shaking )= 0.0004540295119182747\n",
      "LM_prob( hand | the )= 0.0001579778830963665\n",
      "LM_prob( of | hand )= 0.00022660321776569228\n",
      "LM_prob( the | of )= 0.04096638655462185\n",
      "LM_prob( woman | the )= 0.00047393364928909954\n",
      "LM_prob( END | woman )= 0.0018013960819635217\n",
      "p( ['the', 'man', 'got', 'it', 'and', 'put', 'the', 'cigarette', 'off', 'went', 'to', 'the', 'hallway', 'shaking', 'the', 'hand', 'of', 'the', 'woman'] | ['adam', 'bunu', 'anladı', 'sigarasını', 'söndürdü', 'kadının', 'elini', 'sıkarak', 'hole', 'çıktı'] ) = 1.259505044759074e-115 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "####### PHRASE BASED MODEL #######\n",
    "# alignment e:[f]\n",
    "examples_a = [{0: {0:[0], 1:[0], 2:[2], 3:[1], 4:[4], 5:[3], 6:[3], 7:[9], 8:[8], 9:[8], 10:[8], 11:[7], 12:[7], 13:[5], 14:[5],15:[6]}, 1: {0:[0], 1:[0], 2:[2], 3:[1], 4:[4], 5:[3], 6:[3], 7:[9], 8:[8], 9:[8], 10:[8], 11:[7], 12:[7], 13:[6], 14:[5],15:[5],16:[5]}, 2: {0:[0], 1:[0], 2:[2], 3:[1], 4:[4], 5:[4], 6:[3], 7:[3], 8:[4], 9:[9], 10:[8], 11:[8], 12:[8], 13:[7], 14:[6],15:[6],16:[5],17:[5],18:[5]}},\\\n",
    "{0: {0:[2], 1:[1], 2:[2], 3:[0], 4:[0], 5:[3,4,5], 6:[3,4,5], 7:[7], 8:[10], 9:[10], 10:[9], 11:[9], 12:[8], 13:[9], 14:[6]}, 1: {0:[2], 1:[1], 2:[2], 3:[0], 4:[0], 5:[5], 6:[5], 7:[5], 8:[4], 9:[4], 10:[7], 11:[6], 12:[10], 13:[10], 14:[9],15:[9],16:[8],17:[9]}, 2: {0:[2], 1:[1], 2:[2], 3:[0], 4:[0], 5:[3,4,5], 6:[3,4,5], 7:[3,4,5], 8:[7], 9:[6], 10:[10], 11:[10], 12:[10], 13:[10], 14:[9],15:[9],16:[8],17:[9]}, 3: {0:[4], 1:[3,4,5], 2:[3,4,5], 3:[3,4,5], 4:[3,4,5], 5:[2], 6:[1], 7:[1], 8:[1], 9:[2], 10:[7], 11:[10], 12:[6], 13:[10], 14:[9],15:[9],16:[8],17:[9]}},\\\n",
    "{0: {0:[1], 1:[0], 2:[0], 3:[3], 4:[3], 5:[3], 6:[2], 7:[2], 8:[5], 9:[4], 10:[4], 11:[18,19,20], 12:[18,19,20], 13:[6], 14:[7],15:[9],16:[10],17:[11],18:[12,13],19:[14],20:[14,15],21:[17],22:[17]}, 1: {0: [1], 1: [1], 2: [1], 3: [0], 4: [0], 5: [0], 6: [4,5], 7: [4,5], 8: [3], 9: [2], 10: [18,19,20], 11: [6],  12: [6], 13: [7], 14: [7], 15: [8], 16: [8], 17: [10], 18: [9], 19: [10], 20: [11], 21: [11], 22: [13],23:[12,13],24:[16],25:[14,15],26:[14,15],27:[14,15],28:[17],29:[17],30:[17]}}]\n",
    "\n",
    "# use IBM Model 2 for Viterbi alignments & combine them\n",
    "\n",
    "all_a_train = {}\n",
    "for si in range(0, len(f_train)):\n",
    "    e2f = viterbi_alignment(e_train[si], f_train[si], t_e2f, a_e2f)\n",
    "    f2e = viterbi_alignment(f_train[si], e_train[si], t_f2e, a_f2e)\n",
    "    a = combine(f2e, e2f)\n",
    "    all_a_train[si] = a\n",
    "all_a_train[len(f_train)-1] = examples_a[0][0]\n",
    "# extract phrases & estimate phrase translation probabilities\n",
    "\n",
    "max_phrase_len = 8\n",
    "phrase_counts = phrase_extraction(e_train, f_train, all_a_train, max_phrase_len)\n",
    "\n",
    "# train bigram language model LM\n",
    "unigram_counts, bigram_counts = count_grams(e_train)\n",
    "\n",
    "# testing\n",
    "# aligned_phrases format [{index: {e_to: (e_from, f_from, f_to)}}\n",
    "aligned_phrases = [{0: {1:(0,0,0), 2:(2,2,2), 3:(3,1,1), 6:(4,3,4), 10:(7,8,9), 15:(11,5,7)}, 1: {1:(0,0,0), 2:(2,2,2), 3:(3,1,1), 6:(4,3,4), 10:(7,8,9), 16:(11,5,7)}, 2: {1:(0,0,0), 2:(2,2,2), 3:(3,1,1), 8:(4,3,4), 12:(9,8,9), 18:(13,5,7)}}]\n",
    "for sentence in range(0, 1):#len(examples_f)):#TODO for all sentences & not only first after I filled aligned_phrases\n",
    "    f = examples_f[sentence]\n",
    "    for index, e in examples_e[sentence].items():\n",
    "        a = examples_a[sentence][index]\n",
    "        phrases_a = aligned_phrases[sentence][index]\n",
    "        prob = prob_e_given_f(e, f, phrase_counts, unigram_counts, bigram_counts, phrases_a)\n",
    "        print('p(', e, '|', f, ') =', prob,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
