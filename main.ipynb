{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run '/home/lena/Desktop/Application Project/src/ibm1.py'\n",
    "%run '/home/lena/Desktop/Application Project/src/ibm2.py'\n",
    "%run '/home/lena/Desktop/Application Project/src/pbmt.py'\n",
    "%run '/home/lena/Desktop/Application Project/src/utils.py'\n",
    "%run '/home/lena/Desktop/Application Project/src/test.py'\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "p( ['the'] | ['das'] ) = 1.0\n",
      "p( ['the', 'the', 'house', 'a', 'book'] | ['das', 'ein', 'Buch', 'Haus'] ) = 0.0011531999841871323\n",
      "p( ['book', 'a'] | ['Buch', 'ein'] ) = 0.24999999997398636\n",
      "p( ['book', 'the'] | ['Buch', 'ein'] ) = 2.601366715804536e-11\n",
      "p( ['kgefw', 'fek'] | ['Buch', 'ein'] ) = 0.0\n",
      "IBM Model 1 : 40 % correct\n",
      "IBM Model 2 : 0 % correct\n"
     ]
    }
   ],
   "source": [
    "TEST = True \n",
    "nr_used_sentences = 100 #1800 for 1model,25steps  # of 688670 lines\n",
    "path = \"./src/Paralel Corpus/\"\n",
    "e, f = read_in_corpus(nr_used_sentences, path)\n",
    "\n",
    "training_split = 0.9\n",
    "e_train, e_test = split_dataset(e, training_split)\n",
    "f_train, f_test = split_dataset(f, training_split)\n",
    "\n",
    "if TEST:\n",
    "    e_train, f_train, e_test, f_test, all_alignments_test = initialize_test_sets()\n",
    "\n",
    "max_steps = 25\n",
    "\n",
    "t_e2f_ibm1 = EM_IBM_Model_1(e_train, f_train, max_steps)\n",
    "t_e2f, a_e2f = EM_IBM_Model_2(e_train, f_train, t_e2f_ibm1, max_steps)\n",
    "print('------')\n",
    "t_f2e_ibm1 = EM_IBM_Model_1(f_train, e_train, max_steps)\n",
    "t_f2e, a_f2e = EM_IBM_Model_2(f_train, e_train, t_f2e_ibm1, max_steps)\n",
    "\n",
    "#if TEST:\n",
    "    #compare_ibm_1(t_e2f, max_steps, e_train, f_train)\n",
    "    #compare_ibm_2(t, max_steps, a, e_train, f_train)\n",
    "    #compare_ibm_2(t, max_steps, a, f_train, e_train)\n",
    "\n",
    "sum = 0\n",
    "for i in range(len(f_test)):\n",
    "    if (i,4,2,2) in a_f2e:\n",
    "        sum += a[(i,4,2,2)]\n",
    "   # else:\n",
    "    #    print('tüdelü')\n",
    "#print(sum,f_test[i],e_test[4],' TODO sum is supposed to always be 1')\n",
    "\n",
    "# Testing\n",
    "#TODO p(e|f) calc is wrong!!!!\n",
    "wrong1 = 0\n",
    "wrong2 = 0\n",
    "for i in range(0, len(f_test)):\n",
    "    f = f_test[i]\n",
    "    e = e_test[i]\n",
    "    p1 = prob_e_given_f_1(e, f, 1, t_e2f)\n",
    "    if (p1 < 0.01):\n",
    "        wrong1 += 1\n",
    "    print('p(', e, '|', f, ') =', p1)\n",
    "    p2 = 0#prob_e_given_f_2(e, f, 1, t, a)\n",
    "    if (p2 < 0.5):\n",
    "        wrong2 += 1\n",
    "    #print('a: p(', e, '|', f, ') =', p2)\n",
    "    \n",
    "print('IBM Model 1 :', int(100*(1-wrong1/len(f_test))),'% correct')\n",
    "print('IBM Model 2 :', int(100*(1-wrong2/len(f_test))),'% correct')\n",
    "        \n",
    "# arg max_e p(e)p(f|e) decoding manually #TODO how???\n",
    "# calculate p(e|f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the'] ['das']\n",
      "e2f {0: 0}\n",
      "f2e {0: 0}\n",
      "combine {0: [0]}\n",
      "compare aligns True {0: [0]} {0: [0]}\n",
      "-----\n",
      "['the', 'the', 'house', 'a', 'book'] ['das', 'ein', 'Buch', 'Haus']\n",
      "e2f {0: 0, 1: 0, 2: 3, 3: 1, 4: 2}\n",
      "f2e {0: 0, 1: 3, 2: 4, 3: 2}\n",
      "combine {0: [0], 1: [3], 2: [4], 3: [2]}\n",
      "compare aligns False {0: [0], 1: [3], 2: [4], 3: [2]} {0: [0, 1], 1: [3], 2: [4], 3: [2]}\n",
      "-----\n",
      "['book', 'a'] ['Buch', 'ein']\n",
      "e2f {0: 0, 1: 1}\n",
      "f2e {0: 0, 1: 1}\n",
      "combine {0: [0], 1: [1]}\n",
      "compare aligns True {0: [0], 1: [1]} {0: [0], 1: [1]}\n",
      "-----\n",
      "['book', 'the'] ['Buch', 'ein']\n",
      "e2f {0: 0, 1: 0}\n",
      "f2e {0: 0, 1: 0}\n",
      "combine {0: [0], 1: []}\n",
      "compare aligns False {0: [0], 1: []} {0: [0]}\n",
      "-----\n",
      "['kgefw', 'fek'] ['Buch', 'ein']\n",
      "e2f {0: None, 1: None}\n",
      "f2e {0: None, 1: None}\n",
      "combine {}\n",
      "compare aligns True {} {}\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "# Phrase - based MT\n",
    "\n",
    "def word_alignment(e, f, t_e2f, a_e2f, t_f2e, a_f2e):\n",
    "    all_a = {}\n",
    "    for si in range(0, len(f)):\n",
    "        e2f = viterbi_alignment(e[si], f[si], t_e2f, a_e2f)\n",
    "        f2e = viterbi_alignment(f[si], e[si], t_f2e, a_f2e)\n",
    "        print(e[si],f[si])\n",
    "        print('e2f',e2f)\n",
    "        print('f2e',f2e)\n",
    "        a = combine(f2e, e2f)\n",
    "        print('combine', a)\n",
    "        print('compare aligns',a == all_alignments_test[si], a, all_alignments_test[si])\n",
    "        #TODO which behaviour do I want?\n",
    "        #case word has never been seen before > \n",
    "        #case word doesn't have probability with other word >\n",
    "        all_a[si] = a\n",
    "        print('-----')\n",
    "    return all_a\n",
    "\n",
    "# IBM Model 2 for word alignment\n",
    "# TODO actually use e_train but here e_test only to test\n",
    "all_a_test = word_alignment(e_test, f_test, t_e2f, a_e2f, t_f2e, a_f2e)\n",
    "#In my first approach I had implemented the word alignment using the alignment matrices as in the lecture. That turned out as a reason for unnecessary big memory usage and so I decided instead to use a dict as a datastructure with a mapping from index to list of indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"(['the'], ['das'])\": 2, \"(['the'], ['das', 'ein'])\": 1, \"(['the', 'the', 'house', 'a'], ['das', 'ein', 'Buch', 'Haus'])\": 1, \"(['the', 'the', 'house', 'a', 'book'], ['das', 'ein', 'Buch', 'Haus'])\": 1, \"(['the'], ['Haus'])\": 1, \"(['the', 'house'], ['Haus'])\": 1, \"(['the', 'house', 'a'], ['Buch', 'Haus'])\": 1, \"(['the', 'house', 'a'], ['ein', 'Buch', 'Haus'])\": 1, \"(['the', 'house', 'a', 'book'], ['Buch', 'Haus'])\": 1, \"(['the', 'house', 'a', 'book'], ['ein', 'Buch', 'Haus'])\": 1, \"(['house', 'a'], ['Buch'])\": 1, \"(['house', 'a'], ['ein', 'Buch'])\": 1, \"(['house', 'a', 'book'], ['Buch'])\": 1, \"(['house', 'a', 'book'], ['ein', 'Buch'])\": 1, \"(['a'], ['Buch'])\": 1, \"(['a'], ['ein', 'Buch'])\": 1, \"(['a', 'book'], ['Buch'])\": 1, \"(['a', 'book'], ['ein', 'Buch'])\": 1, \"(['book'], ['Buch'])\": 2, \"(['book', 'a'], ['Buch', 'ein'])\": 1, \"(['a'], ['ein'])\": 1, \"(['book'], ['Buch', 'ein'])\": 1, \"(['book', 'the'], ['Buch'])\": 1, \"(['book', 'the'], ['Buch', 'ein'])\": 1}\n",
      "0\n",
      "0\n",
      "the 0.2\n",
      "book 0.001953125\n",
      "a 0.04938271604938271\n",
      "the 0.024691358024691357\n",
      "fek 0.02040816326530612\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "e_train = [['michael','assumes','that','he','will','stay','in','the','house']]\n",
    "f_train = [['michael','geht','davon','aus',',','dass','er','im','haus','bleibt']]\n",
    "all_a = {0:{0:[0],1:[1,2,3],2:[5],3:[6],4:[9],5:[9],6:[7],7:[7],8:[8]}}\n",
    "# all_a[a][b] alignment for b word in e for sentence a\n",
    "\"\"\"\n",
    "\n",
    "#TODO do for train\n",
    "#all_BP = phrase_extraction(e_train, f_train, all_a_train)\n",
    "all_BP = phrase_extraction(e_test, f_test, all_a_test)\n",
    "\n",
    "# probabilistic phrase translation table\n",
    "# For each sentence pair, we extract a number of phrase pairs.\n",
    "# Then, we count in how many sentence pairs a particular phrase pair is\n",
    "# extracted and store this number in count(e,f).\n",
    "counts = {}\n",
    "for (a, b) in all_BP:\n",
    "\t# key = (frozenset(a),frozenset(b))\n",
    "\tkey = str((a, b))\n",
    "\tcounts[key] = counts.get(key, 0) + 1\n",
    "print(counts)\n",
    "\n",
    "#TODO phrase translation table\n",
    "print(φ(['house'], ['Haus'], counts))\n",
    "print(φ(['the','house'],['das','Haus'],counts))\n",
    "\n",
    "#bigram language model\n",
    "unigram_counts, bigram_counts = count_grams(e_test)\n",
    "prev = 'the'\n",
    "cur = 'book'\n",
    "#print(bigram_prob(prev, cur, unigram_counts, bigram_counts))\n",
    "cur = 'a'\n",
    "#print(bigram_prob(prev, cur, unigram_counts, bigram_counts))\n",
    "\n",
    "# TODO phrase-based statistical machine translation model\n",
    "# • the phrase translation table φ(f |e);\n",
    "# • the reordering model d;\n",
    "# • the language model pLM(e).\n",
    "# e_best = argmax_e prod_i φ(f|e) * d(start_i - end_i_min_1 - 1) * prod_i PLM(e_i|e_i_min_1)\n",
    "for sentence in range(0, len(all_alignments_test)):\n",
    "    f = f_test[sentence]\n",
    "    e = e_test[sentence]\n",
    "    alignment = all_alignments_test[sentence]\n",
    "    max_value = 0 \n",
    "    e_best = None\n",
    "    for j in range(0, len(e)):\n",
    "        e_j = e[j]\n",
    "        start_j, end_j_min_1 = calc_vals_for_d(alignment, j, f)\n",
    "        phrase_transl = 1\n",
    "        PLM = 1\n",
    "        for i in range(0, len(f)):\n",
    "            #phrase_transl *= φ(e_j,f[i], counts) #TODO always 0\n",
    "            if j > 0: prev = e[j-1] #TODO wrong, I dont get it\n",
    "            else: prev = e[j]\n",
    "            PLM *= bigram_prob(prev, e_j, unigram_counts, bigram_counts)\n",
    "        value = phrase_transl * d(start_j - end_j_min_1 - 1) * PLM\n",
    "        if value > max_value:\n",
    "            max_value = value \n",
    "            e_best = e_j\n",
    "    print(e_best, max_value)\n",
    "            \n",
    "        #print(j, start_j, end_j_min_1, start_j - end_j_min_1 - 1)  #to test reordering\n",
    "#todo implement max_phrase_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODOS\n",
    "\n",
    "#do summary 09!\n",
    "\n",
    "#TODO unaligned cases!\n",
    "\n",
    "#TODO wrong, use start and end! in counts!\n",
    "\n",
    "#debug ibm 2\n",
    "#IBM Model 2: How to calculate a correctly?\n",
    "#my a is 1 when it should be low :(\n",
    "#implementation exactly like in book\n",
    "#eg sum is supposed to always be 1\n",
    "#use ibm model 2 in slides\n",
    "#compare subresults if they are right, count, stotal etc\n",
    "#look into indexing, starting 0 / 1?\n",
    "\n",
    "# print(φ(['house'], ['Haus'], count))\n",
    "# print(φ(['the','house'],['das','Haus']))\n",
    "# phrase translation table doesnt work, always returns 0\n",
    "\n",
    "# TODO translation probability is not working for phrases containing several words\n",
    "#p(e|f) for IBM Model 2: Gives result 4 should be 1?\n",
    "#is sum wrong maybe?\n",
    "#> compare with other implementations\n",
    "\n",
    "#calculating p(e|f) phrase based model\n",
    "\n",
    "#decoding > Nazmi\n",
    "#generate some example target sentences (word sequences) “manually”\n",
    "#&gen alignments\n",
    "\n",
    "#TODO save t, a in files & load them?\n",
    "\n",
    "#structure code more? classes ML model, so it's intuitive to execute\n",
    "\n",
    "#calc how long stuff takes\n",
    "#look into efficiency issues, use hashing?\n",
    "# TODO For the estimation of the phrase translation probabilities, not all\n",
    "# phrase pairs have to be loaded into memory. It is possible to efficiently\n",
    "# estimate the probability distribution by storing and sorting the extracted\n",
    "# phrases on disk. Similarly, when using the translation table for the translation of a single sentence, only a small fraction of it is needed and may\n",
    "# be loaded on demand.\n",
    "# think about saving stuff in file\n",
    "# • Phrase translation table typically bigger than corpus\n",
    "# ... even with limits on phrase lengths (e.g., max 7 words)\n",
    "# → Too big to store in memory?\n",
    "# • Solution for training\n",
    "# – extract to disk, sort, construct for one source phrase at a time\n",
    "# • Solutions for decoding\n",
    "# – on-disk data structures with index for quick look-ups\n",
    "# – suffix arrays to create phrase pairs on demand\n",
    "# TODO use numpy for everything\n",
    "# maybe other data structure ['the','house']:[['das','Haus'],['ein','Haus']]\n",
    "# faster sometimes but slower if translation in other direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-8d0de783cea0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnr_used_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1800\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./src/Paralel Corpus/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_in_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnr_used_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtraining_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Application Project/src/utils.py\u001b[0m in \u001b[0;36mread_in_corpus\u001b[0;34m(nr_used_sentences, corpus_path)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0me_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mf_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfilename_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8-sig'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mf_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_lines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnr_used_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "####### IBM MODELS #######\n",
    "\n",
    "# read in data\n",
    "nr_used_sentences = 1800\n",
    "path = \"./src/Paralel Corpus/\"\n",
    "e, f = read_in_corpus(nr_used_sentences, path)\n",
    "\n",
    "training_split = 0.9\n",
    "e_train, e_test = split_dataset(e, training_split)\n",
    "f_train, f_test = split_dataset(f, training_split)\n",
    "    \n",
    "max_steps = 25\n",
    "\n",
    "# execute IBM Model 1 and 2 each two times (English-Turkish and Turkish-English)\n",
    "t_e2f_ibm1 = EM_IBM_Model_1(e_train, f_train, max_steps)\n",
    "t_e2f, a_e2f = EM_IBM_Model_2(e_train, f_train, t_e2f_ibm1, max_steps)\n",
    "t_f2e_ibm1 = EM_IBM_Model_1(f_train, e_train, max_steps)\n",
    "t_f2e, a_f2e = EM_IBM_Model_2(f_train, e_train, t_f2e_ibm1, max_steps)\n",
    "\n",
    "# testing\n",
    "''''\n",
    "for i in range(0, len(f_test)):\n",
    "    f = f_test[i]\n",
    "    e = e_test[i]\n",
    "    print(e)\n",
    "    p1 = prob_e_given_f_1(e, f, 1, t_f2e_ibm1)\n",
    "    print('IBM Model 1 :p(', e, '|', f, ') =', p1)\n",
    "    p2 = prob_e_given_f_2(e, f, 1, t_f2e, a_f2e)\n",
    "    print('IBM Model 2: p(', e, '|', f, ') =', p2)\n",
    "''''\n",
    "examples_f = [['adam', 'bunu', 'anladı', 'sigarasını', 'söndürdü', 'kadının', 'elini', 'sıkarak', 'hole', 'çıktı']]#,['girdiği', 'bütün', 'işlerde', 'bir', 'terslikle', 'karşılaşmış', 'şimdi', 'de', 'demiryolu', 'işletmesine', 'geçmişti'], ['duruşma', 'aralarında', 'çayım', 'içip', 'sigarasını', 'tüttürerek', 'biraz', 'siyasetten', 'biraz', 'günlük', 'işlerden', 'biraz', 'kâğıt', 'oyunlarından', 'daha', 'çok', 'da', 'atamalardan', 'söz', 'açtığı', 'olurdu']]\n",
    "examples_e = [['the' ,'man', 'understood','this','and','extinguished','his','cigarette','on','the','womans','hand']]\n",
    "#He had ruined his prospects in a number of positions and was not serving in the railway department. \n",
    "#In the intervals between the sessions he smoked, drank tea, chatted a little about politics, a little about general topics, a little about cards, but most of all about official appointments. \n",
    "for i in range(0, len(examples_f)):\n",
    "    f = examples_f[i]\n",
    "    e = examples_e[i]\n",
    "    p1 = prob_e_given_f_1(e, f, 1, t_f2e_ibm1)\n",
    "    print('IBM Model 1 :p(', e, '|', f, ') =', p1)\n",
    "    p2 = prob_e_given_f_2(e, f, 1, t_f2e, a_f2e)\n",
    "    print('IBM Model 2: p(', e, '|', f, ') =', p2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### PHRASE BASED MODEL #######\n",
    "\n",
    "# use IBM Model 2 for Viterbi alignments & combine them\n",
    "\n",
    "all_a_train = {}\n",
    "for si in range(0, len(f_train)):\n",
    "    e2f = viterbi_alignment(e_train[si], f_train[si], t_e2f, a_e2f)\n",
    "    f2e = viterbi_alignment(f_train[si], e_train[si], t_f2e, a_f2e)\n",
    "    a = combine(f2e, e2f)\n",
    "    all_a_train[si] = a\n",
    "\n",
    "# extract phrases & estimate phrase translation probabilities\n",
    "\n",
    "max_phrase_len = 5 #TODO\n",
    "all_BP = phrase_extraction(e_train, f_train, all_a_train, max_phrase_len)\n",
    "phrase_counts = count_phrases(all_BP)\n",
    "\n",
    "# train bigram language model LM\n",
    "\n",
    "unigram_counts, bigram_counts = count_grams(e_train)\n",
    "\n",
    "# testing\n",
    "\n",
    "examples_f = [['adam', 'bunu', 'anladı', 'sigarasını', 'söndürdü', 'kadının', 'elini', 'sıkarak', 'hole', 'çıktı']]#,['girdiği', 'bütün', 'işlerde', 'bir', 'terslikle', 'karşılaşmış', 'şimdi', 'de', 'demiryolu', 'işletmesine', 'geçmişti'], ['duruşma', 'aralarında', 'çayım', 'içip', 'sigarasını', 'tüttürerek', 'biraz', 'siyasetten', 'biraz', 'günlük', 'işlerden', 'biraz', 'kâğıt', 'oyunlarından', 'daha', 'çok', 'da', 'atamalardan', 'söz', 'açtığı', 'olurdu']]\n",
    "examples_e = [['the' ,'man', 'understood','this', 'and','extinguished','his','cigarette','leaving','a','hole','on','the','womans','hand']]\n",
    "#He had ruined his prospects in a number of positions and was not serving in the railway department. \n",
    "#In the intervals between the sessions he smoked, drank tea, chatted a little about politics, a little about general topics, a little about cards, but most of all about official appointments. \n",
    "examples_a = [[0:[0], 1:[0], 2:[2], 3:[1], 4:[None], 5:[4], 6:[3], 7:[3], 8:[9], 9:[8], 10:[8], 11:[6], 12:[5], 13:[5], 14:[6]]\n",
    "for i in range(0, len(examples_f)):\n",
    "    f = examples_f[i]\n",
    "    e = examples_e[i]\n",
    "    a = examples_a[i]\n",
    "    prob = prob_e_given_f(e, f, a)\n",
    "    print('p(', e, '|', f, ') =', prob)\n",
    "              \n",
    "# e_best = argmax_e prod_i φ(f|e) * d(start_i - end_i_min_1 - 1) * prod_i PLM(e_i|e_i_min_1)\n",
    "def prob_e_given_f(e, f, a, phrase_counts, unigram_counts, bigram_counts)\n",
    "    max_value = 0\n",
    "    e_best = None\n",
    "    for j in range(0, len(e)):\n",
    "        e_j = e[j]\n",
    "        #TODO how to calc prob for sentence / how to for word?\n",
    "        PT_prob = PT_prob(e_j, f[i], phrase_counts)\n",
    "        prev = 0 #TODO wrong, use start and end! in counts!\n",
    "        LM_prob = LM_prob(prev, e_j, unigram_counts, bigram_counts)\n",
    "        start_j, end_j_min_1 = calc_vals_for_d(a, j, f)\n",
    "        d = d(start_j - end_j_min_1 - 1)\n",
    "        prob = PT_prob * d * LM_prob\n",
    "    return prob"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
